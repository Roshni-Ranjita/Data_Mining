{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31551102-0d1d-4782-94a1-af4860fe322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2368b174-c2ee-4fc8-b547-e5bdc0e6ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "adj = sp.load_npz(\"../data/data/adj.npz\")\n",
    "features = np.load(\"../data/data/features.npy\")\n",
    "labels = np.load(\"../data/data/labels.npy\")\n",
    "with open(\"../data/data/splits.json\") as f:\n",
    "    splits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ad18d84-7c57-4c6d-902e-6e650bbdcd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (2480, 2480)\n",
      "Features shape: (2480, 1390)\n",
      "Labels shape: (496,)\n",
      "Train set size: 496, Test set size: 1984\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adjacency matrix shape: {adj.shape}\")\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Train set size: {len(splits['idx_train'])}, Test set size: {len(splits['idx_test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bed40c75-f651-476e-9540-e02545f9e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy array (in case it's a list)\n",
    "idx_train = np.array(splits['idx_train'])\n",
    "idx_test_a = np.array(splits['idx_test'])\n",
    "\n",
    "# Extract training features\n",
    "X_train_features = pd.DataFrame(features[idx_train]).reset_index(drop=True)\n",
    "X_test_features = pd.DataFrame(features[idx_test]).reset_index(drop=True)\n",
    "\n",
    "X_train_adj = pd.DataFrame(adj[idx_train]).reset_index(drop=True)\n",
    "X_test_adj = pd.DataFrame(adj[idx_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be1601-334b-4f7e-bb96-0b7721972da0",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f66da277-270d-4f54-98f3-c11dedf14eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec486e02-d807-49fc-b5ad-4a75f7f5cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2051, 1788, 1233, 926, 2053, 2083, 2370, 1306, 1354, 603, 571, 1132, 766, 789, 108, 1196, 2462, 2043, 293, 1626, 2376, 1619, 2399, 1887, 467, 1449, 2200, 2425, 2128, 1304, 1613, 1596, 1003, 1645, 1537, 2476, 2358, 33, 73, 1010, 586, 1825, 1880, 1666, 612, 1521, 10, 2254, 1539, 311, 1640, 860, 187, 2417, 878, 1791, 523, 90, 1663, 2084, 1225, 246, 1223, 405, 1042, 864, 297, 1827, 2352, 729, 1032, 1846, 1953, 2409, 1309, 442, 1276, 1702, 906, 2336, 327, 549, 354, 740, 1792, 1715, 370, 1013, 216, 56, 1633, 1116, 2205, 745, 854, 1469, 944, 1379, 493, 1093, 402, 1385, 2294, 748, 1898, 313, 1155, 2247, 2070, 2235, 2363, 1401, 845, 1427, 364, 1858, 1103, 1870, 298, 384, 1681, 1957, 1495, 805, 2314, 68, 96, 2119, 1012, 1584, 1722, 2320, 1453, 1302, 1538, 1481, 1100, 440, 865, 528, 2427, 2478, 489, 1027, 704, 2010, 1817, 1074, 2284, 771, 797, 1377, 1487, 151, 65, 1305, 2242, 366, 798, 817, 1275, 1946, 2178, 682, 23, 1774, 1959, 2007, 791, 2199, 2177, 2069, 91, 962, 2145, 1972, 1217, 2123, 1263, 470, 2087, 340, 557, 947, 1782, 1849, 1099, 1308, 822, 1876, 1814, 2155, 985, 796, 1360, 2259, 1388, 1929, 1035, 721, 309, 95, 1672, 400, 889, 2262, 917, 2025, 1851, 2228, 1952, 1657, 2215, 1964, 762, 31, 2184, 2410, 2013, 1293, 733, 660, 1830, 290, 383, 2122, 1821, 1855, 355, 420, 2319, 1693, 304, 256, 648, 1137, 2280, 170, 2423, 2282, 1395, 1163, 1994, 819, 2213, 1516, 1926, 1149, 1069, 765, 2212, 1859, 1668, 2018, 1566, 1353, 1270, 1685, 1689, 2011, 1762, 1575, 910, 1460, 1454, 436, 322, 1600, 2304, 1527, 780, 1452, 1496, 2014, 969, 1091, 982, 2040, 1235, 2144, 1740, 512, 1210, 224, 595, 282, 711, 563, 957, 927, 507, 156, 1201, 213, 1462, 1699, 1111, 589, 1226, 687, 1676, 97, 1643, 1654, 1540, 2118, 281, 795, 64, 1834, 1097, 1642, 678, 344, 2346, 2169, 1989, 1556, 1430, 148, 2221, 250, 996, 1655, 509, 252, 2386, 407, 2104, 323, 2163, 315, 222, 1218, 951, 1077, 1030, 2151, 497, 628, 1737, 2375, 616, 75, 546, 1503, 1351, 469, 1886, 233, 181, 2273, 851, 1941, 502, 1122, 1974, 464, 1857, 1194, 543, 30, 650, 270, 585, 50, 481, 15, 1881, 2274, 874, 2224, 1546, 2004, 1726, 519, 385, 2194, 561, 1292, 907, 862, 1089, 1621, 234, 1635, 1051, 977, 601, 1050, 813, 1954, 542, 1882, 1550, 1389, 1445, 608, 278, 610, 1332, 48, 1124, 1041, 1552, 2146, 415, 918, 1939, 2313, 882, 468, 545, 167, 1869, 1987, 1020, 1812, 2387, 1374, 63, 264, 1502, 2450, 314, 1333, 2251, 1597, 1803, 1634, 1166, 2442, 902, 2379, 460, 2394, 421, 1535, 373, 403, 2297, 2171, 1945, 1564, 1229, 619, 1873, 587, 831, 884, 2468, 1599, 1490, 1649, 368, 2391, 1458, 1936, 236, 39, 1879, 1977, 2137, 786, 1024, 2389, 450, 312, 1983, 494, 1703, 1164, 1232, 993, 1958, 1062, 1602, 2015, 1690, 2150, 386, 1969, 1960, 1450, 1121, 1437, 1438, 433, 1839, 2473, 1126, 2075, 1238, 810, 1072, 38, 511, 2414, 2329, 574, 827, 411, 1720, 2160, 472, 1973, 2305, 1593, 1622, 112, 158, 583, 2125, 117, 1117, 416, 1532, 375, 2474, 2185, 110, 1115, 465, 1981, 1580, 2103, 644, 379, 974, 2176, 474, 2234, 382, 2093, 784, 716, 207, 2434, 1620, 794, 2256, 2356, 2285, 886, 51, 1337, 341, 376, 724, 1328, 1123, 408, 1108, 750, 466, 1288, 570, 1805, 1497, 2120, 1476, 1468, 1644, 461, 1775, 686, 892, 531, 1542, 527, 2114, 1560, 807, 1725, 522, 491, 324, 206, 1914, 834, 2218, 1067, 2245, 2236, 2372, 2265, 626, 325, 360, 2055, 1579, 1101, 229, 147, 2420, 2147, 1425, 1828, 136, 1339, 2026, 2299, 1426, 1433, 1028, 2162, 2095, 651, 2388, 1207, 1463, 1976, 1038, 2298, 1978, 1181, 2219, 701, 928, 2260, 1322, 1046, 1206, 2413, 1850, 1604, 449, 2342, 2041, 621, 2019, 700, 581, 2050, 342, 1808, 1517, 1783, 1761, 588, 1285, 1905, 1918, 253, 477, 1066, 1785, 381, 496, 2255, 642, 296, 14, 671, 2323, 1794, 1367, 1773, 2272, 1191, 777, 1709, 665, 1986, 1380, 2377, 1371, 205, 961, 2192, 743, 673, 1847, 1829, 62, 1008, 894, 1170, 1443, 2198, 111, 2443, 2333, 28, 1168, 369, 1061, 100, 932, 2422, 1336, 2318, 1144, 1017, 1662, 1039, 1823, 1106, 361, 709, 2180, 1591, 2074, 558, 1899, 746, 1912, 29, 775, 1548, 32, 2193, 613, 2316, 89, 4, 16, 272, 593, 214, 1127, 2067, 1318, 2328, 534, 367, 2458, 1025, 1286, 18, 1227, 413, 225, 1734, 223, 2209, 251, 1682, 274, 275, 2437, 1806, 2310, 2469, 1893, 767, 763, 602, 1146, 276, 562, 1738, 1647, 1943, 2037, 7, 424, 749, 1993, 2072, 1653, 869, 94, 202, 1224, 1673, 1811, 1246, 1382, 1400, 1475, 1157, 378, 997, 2345, 78, 706, 144, 1082, 1054, 1603, 2452, 1947, 2059, 1522, 615, 2096, 779, 0, 525, 1019, 1404, 190, 1260, 1718, 2020, 1175, 1419, 57, 321, 2124, 163, 2174, 2208, 245, 2079, 1486, 1506, 2166, 710, 742, 853, 1748, 1674, 1136, 1577, 126, 446, 1314, 1797, 2308, 55, 1413, 2411, 999, 232, 25, 726, 1415, 924, 300, 1183, 690, 1378, 2349, 1047, 1810, 457, 1924, 508, 329, 1222, 1742, 866, 1465, 734, 1244, 2311, 1448, 1257, 1000, 1245, 787, 1262, 1543, 302, 1178, 966, 632, 101, 268, 1280, 2230, 1242, 2204, 1648, 2365, 1184, 2433, 2154, 2325, 903, 643, 154, 1096, 1688, 1331, 1853, 1636, 1717, 479, 132, 597, 559, 1772, 36, 372, 730, 1480, 1301, 1650, 1860, 1081, 1601, 518, 1056, 981, 1967, 736, 592, 82, 756, 1605, 1362, 694, 1021, 1961, 715, 2402, 1125, 149, 1418, 1624, 1755, 1728, 88, 2202, 2143, 115, 118, 2392, 1816, 1884, 606, 1840, 1984, 1614, 1659, 1340, 965, 922, 129, 1391, 338, 1948, 2022, 2206, 1009, 1078, 1482, 1751, 1329, 1592, 2249, 1683, 221, 305, 1731, 2269, 670, 1949, 67, 770, 159, 487, 504, 1530, 1697, 2211, 1864, 1889, 552, 141, 2477, 567, 925, 1598, 462, 2415, 2288, 800, 480, 1863, 577, 2460, 46, 893, 1789, 609, 2270, 1169, 371, 1031, 1204, 406, 1745, 320, 211, 1086, 1658, 1182, 1186, 858, 1114, 1303, 1167, 1625, 1678, 808, 74, 255, 1131, 1092, 2186, 168, 2107, 1148, 1064, 2188, 868, 1652, 2045, 980, 1271, 429, 684, 2179, 319, 267, 2364, 2129, 727, 1545, 514, 45, 419, 2266, 106, 105, 1055, 1639, 2400, 2196, 1057, 912, 142, 2428, 761, 269, 1214, 1139, 1589, 445, 548, 1979, 271, 1923, 451, 1753, 2105, 801, 1807, 299, 804, 913, 713, 2436, 520, 2322, 418, 2337, 649, 718, 2140, 86, 1073, 1578, 2133, 2445, 2000, 1409, 160, 1970, 1034, 13, 1464, 685, 915, 125, 654, 410, 395, 2398, 482, 182, 2109, 1347, 121, 1710, 145, 991, 425, 995, 2057, 672, 723, 560, 199, 217, 47, 212, 219, 666, 1243, 1098, 1372, 292, 1177, 2303, 428, 954, 1173, 1160, 1571, 747, 475, 1198, 2006, 2035, 1795, 2156, 2432, 1766, 1310, 1769, 630, 1854, 196, 1118, 2167, 792, 143, 1442, 2060, 1541, 365, 2446, 119, 1444, 12, 1384, 1990, 1355, 2354, 174, 2397, 1519, 1026, 294, 2335, 1510, 821, 58, 653, 353, 265, 1090, 1249, 1995, 1344, 412, 741, 1472, 1298, 891, 198, 1992, 1070, 1551, 692, 1770, 758, 455, 1698, 2347, 1457, 1777, 872, 2009, 1515, 387, 883, 150, 1837, 2383, 2085, 1865, 2086, 1583, 1735, 970, 176, 1023, 1691, 2289, 594, 885, 204, 1922, 2330, 1439, 1179, 2416, 538, 1113, 1088, 1119, 1868, 2459, 1033, 815, 598, 1402, 696, 2373, 1802, 1991, 423, 1200, 2097, 2052, 596, 261, 1256, 2464, 349, 1441, 505, 2223, 986, 26, 1696, 940, 1251, 1708, 1282, 661, 579, 989, 1153, 2263, 1950, 171, 2281, 2142, 1561, 1609, 1393, 2100, 2197, 1493, 1687, 849, 2312, 720, 273, 2071, 2027, 841, 2226, 679, 1018, 2152, 1746, 669, 98, 218, 1919, 1434, 958, 1907, 503, 943, 1484, 427, 1315, 1424, 1719, 1110, 76, 500, 1528, 1363, 825, 1809, 2029, 515, 1221, 1500, 2339, 1461, 1141, 1871, 565, 899, 1588, 1623, 950, 388, 239, 1776, 955, 1513, 1269, 1159, 1815, 2126, 2173, 2264, 1877, 1080, 1763, 887, 806, 605, 1423, 1554, 809, 652, 737, 539, 1477, 431, 1629, 1143, 1756, 1747, 1525, 1296, 83, 2047, 1421, 1368, 840, 759, 2068, 1411, 731, 582, 973, 2291, 2036, 1664, 725, 938, 1029, 1253, 1767, 1675, 575, 350, 1473, 870, 1841, 390, 2227, 638, 1909, 2021, 823, 1843, 260, 1790, 2301, 1723, 663, 1555, 1754, 2092, 348, 1739, 99, 2031, 1366, 1295, 2287, 243, 197, 998, 1412, 37, 1414, 2048, 2238, 1714, 1573, 842, 220, 2214, 568, 2355, 963, 1338, 2066, 1219, 1383, 85, 430, 2138, 397, 772, 657, 362, 2449, 994, 1466, 301, 633, 1670, 2141, 1299, 636, 2164, 1250, 897, 1533, 540, 2348, 2479, 49, 2153, 1489, 1563, 103, 1520, 2091, 42, 1048, 935, 554, 1359, 1036, 1988, 2182, 839, 1446, 1165, 1436, 444, 537, 1079, 890, 1507, 728, 2135, 80, 1313, 717, 2090, 113, 1628, 1151, 1607, 258, 2024, 332, 138, 2237, 359, 674, 623, 1176, 357, 2457, 242, 668, 2448, 811, 2271, 667, 1999, 2278, 2435, 971, 764, 836, 735, 2106, 456, 1512, 1346, 227, 2191, 1743, 600, 2077, 2233, 277, 1052, 1968, 1364, 1627, 1252, 1112, 2111, 1161, 1261, 1749, 590, 707, 584, 1701, 1209, 140, 2359, 1559, 802, 1680, 875, 131, 41, 1059, 1104, 1220, 2444, 677, 916, 193, 1248, 133, 1677, 1764, 2407, 2470, 393, 1963, 2341, 1786, 1397, 1631, 164, 1234, 2132, 104, 1867, 307, 2049, 1576, 2061, 1514, 93, 1203, 987, 576, 331, 485, 1255, 2102, 693, 2033, 909, 454, 1085, 2390, 1158, 2062, 1781, 880, 1053, 931, 2225, 1526, 732, 948, 401, 2003, 1524, 618, 2439, 2189, 2382, 1518, 1800, 1327, 1523, 1544, 2063, 1660, 135, 1478, 1927, 249, 266, 44, 1819, 336, 2378, 2396, 434, 908, 1686, 1713, 484, 1399, 2222, 1938, 306, 2447, 192, 708, 2426, 374, 1705, 2454, 788, 929, 760, 54, 20, 1145, 2463, 2058, 200, 2008, 1037, 1386, 2296, 1435, 921, 1897, 1793, 1585, 550, 1193, 1937, 979, 1741, 1237, 829, 773, 939, 1932, 1205, 1616, 530, 2429, 2307, 1962, 1933, 1134, 1975, 1890, 2017, 1934, 1692, 53, 1356, 1706, 184, 1330, 1044, 289, 1187, 2380, 2279, 175, 1921, 629, 2076, 70, 1511, 1779, 1547, 622, 1213, 2475, 194, 210, 1394, 919, 933, 604, 599, 389, 782, 2108, 703, 1557, 248, 1955, 2369, 816, 1499, 284, 1494, 1667, 2385, 1001, 1572, 1300, 237, 855, 1084, 1906, 486, 754, 1505, 285, 2134, 356, 1455, 2261, 1732, 2381, 876, 936, 328, 1567, 247, 2466, 856, 157, 942, 17, 859, 751, 43, 914, 1796, 1239, 1940, 2244, 535, 1102, 1928, 785, 1716, 1065, 1264, 235, 2302, 1910, 2350, 1174, 1410, 2044, 2332, 1694, 1727, 2028, 2101, 1838, 1594, 2195, 1283, 40, 1343, 896, 647, 1150, 968, 2293, 832, 1724, 2472, 439, 438, 178, 1268, 1190, 1509, 826, 363, 1595, 1610, 1917, 2327, 1617, 162, 92, 1192, 705, 688, 2042, 1835, 209, 2165, 2038, 2404, 1172, 120, 1784, 1641, 1432, 953, 123, 1833, 1998, 1733, 946, 2023, 2203, 722, 2098, 139, 2393, 2187, 1422, 2056, 443, 1352, 2248, 1915, 956, 1606, 183, 1324, 691, 978, 1428, 345, 1274, 1373, 566, 850, 551, 1109, 569, 66, 240, 2361, 1281, 2149, 422, 2306, 904, 664, 1780, 1798, 1744, 2441, 1120, 2099, 2158, 1195, 2207, 2326, 857, 768, 2241, 1094, 2300, 226, 492, 1325, 1254, 1273, 1015, 1913, 1147, 1608, 553, 2181, 911, 1230, 689, 116, 2089, 847, 1966, 863, 72, 1982, 555, 2357, 2130, 541, 1997, 1892, 640, 627, 1361, 109, 1611, 1211, 2001, 812, 2175, 683, 177, 2183, 1142, 1896, 1323, 2371, 499, 905, 1350, 1156, 52, 1007, 676, 556, 463, 2240, 347, 2054, 1071, 2292, 1240, 1267, 495, 1345, 146, 828, 2039, 778, 835, 11, 1231, 447, 2455, 1570, 24, 478, 279, 1612, 1874, 1138, 1885, 867, 399, 1498, 195, 2317, 2401, 435, 697, 752, 1284, 1581, 2276, 201, 27, 257, 2032, 1804, 838, 2136, 1451, 972, 283, 2338, 1492, 1471, 2309, 352, 2467, 1700, 2088, 81, 625, 1831, 1826, 1358, 920, 1861, 377, 516, 861, 1942, 188, 930, 738, 124, 776, 680, 1289, 1068, 128, 2384, 536, 1704, 2157, 398, 2268, 1063, 2002, 2344, 2438, 134, 755, 189, 2331]\n"
     ]
    }
   ],
   "source": [
    "# Load the adjacency matrix (assumed stored as a sparse matrix)\n",
    "adj = sp.load_npz('../data/data/adj.npz')[idx_train, :][:, idx_train]\n",
    "adj_test = sp.load_npz('../data/data/adj.npz')[idx_test_a, :][:, idx_test_a]\n",
    "\n",
    "# Load node features and labels (assumed to be in numpy array format)\n",
    "features = np.load('../data/data/features.npy')[idx_train]\n",
    "#features_test = np.load('../data/data/features.npy')[idx_test_a]\n",
    "labels = np.load('../data/data/labels.npy')\n",
    "\n",
    "# Load training and testing splits from splits.json\n",
    "with open('../data/data/splits.json') as f:\n",
    "    splits = json.load(f)\n",
    "idx_train = splits['idx_train']\n",
    "idx_test_a = splits['idx_test']\n",
    "print(idx_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf168e73-94ed-4946-a2c2-3b1eed6cdee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Add self-loops to the adjacency matrix and normalize it.\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize the adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    # Add self-loops\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    # Handle inf entries (in case of isolated nodes)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "adj_normalized = normalize_adj(adj)\n",
    "adj_normalized_test = normalize_adj(adj_test)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "333fc4ed-4765-4f8b-bac9-e9baac3f514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert the normalized sparse matrix to a torch sparse tensor.\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "adj_normalized = sparse_mx_to_torch_sparse_tensor(adj_normalized)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bcc02838-1ad4-4cb4-bfd0-bfc979660fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels to torch tensors\n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "#print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73729576-d054-418c-8d0d-751ef24d02da",
   "metadata": {},
   "source": [
    "### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fcdd501-3956-45bb-89b9-80033d65bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution layer as described in Kipf & Welling (2017)\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        # Multiply input features by the weight matrix\n",
    "        support = torch.mm(input, self.weight)\n",
    "        # Perform sparse matrix multiplication with the normalized adjacency matrix\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ecc7559-b8fb-4c42-8385-41dd9bb8fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cffcd7ba-c6bf-41b7-8c40-40aa24ae9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afe855e9-c3cb-492c-a672-567bb116d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj_normalized)\n",
    "    loss_train = F.nll_loss(output, labels)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d566bd8-827f-4cbc-8bc6-0fc3b34b7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj_normalized)\n",
    "    loss_test = F.nll_loss(output, labels)\n",
    "    preds = output.max(1)[1]\n",
    "    correct = preds.eq(labels).sum().item()\n",
    "    acc_test = correct / len(idx_train)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd317b3f-34a8-4de3-85b0-b769eb7ca848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0859\n",
      "Epoch: 0002 loss_train: 1.9267\n",
      "Epoch: 0003 loss_train: 1.8366\n",
      "Epoch: 0004 loss_train: 1.7656\n",
      "Epoch: 0005 loss_train: 1.6795\n",
      "Epoch: 0006 loss_train: 1.6140\n",
      "Epoch: 0007 loss_train: 1.5581\n",
      "Epoch: 0008 loss_train: 1.4623\n",
      "Epoch: 0009 loss_train: 1.4009\n",
      "Epoch: 0010 loss_train: 1.3168\n",
      "Epoch: 0011 loss_train: 1.2766\n",
      "Epoch: 0012 loss_train: 1.2142\n",
      "Epoch: 0013 loss_train: 1.0920\n",
      "Epoch: 0014 loss_train: 1.0226\n",
      "Epoch: 0015 loss_train: 0.9759\n",
      "Epoch: 0016 loss_train: 0.9060\n",
      "Epoch: 0017 loss_train: 0.8802\n",
      "Epoch: 0018 loss_train: 0.7993\n",
      "Epoch: 0019 loss_train: 0.7937\n",
      "Epoch: 0020 loss_train: 0.7567\n",
      "Epoch: 0021 loss_train: 0.6863\n",
      "Epoch: 0022 loss_train: 0.6361\n",
      "Epoch: 0023 loss_train: 0.6404\n",
      "Epoch: 0024 loss_train: 0.5875\n",
      "Epoch: 0025 loss_train: 0.5973\n",
      "Epoch: 0026 loss_train: 0.5476\n",
      "Epoch: 0027 loss_train: 0.5261\n",
      "Epoch: 0028 loss_train: 0.4982\n",
      "Epoch: 0029 loss_train: 0.4758\n",
      "Epoch: 0030 loss_train: 0.4846\n",
      "Epoch: 0031 loss_train: 0.4208\n",
      "Epoch: 0032 loss_train: 0.3763\n",
      "Epoch: 0033 loss_train: 0.4171\n",
      "Epoch: 0034 loss_train: 0.4169\n",
      "Epoch: 0035 loss_train: 0.3820\n",
      "Epoch: 0036 loss_train: 0.3818\n",
      "Epoch: 0037 loss_train: 0.3777\n",
      "Epoch: 0038 loss_train: 0.3790\n",
      "Epoch: 0039 loss_train: 0.3477\n",
      "Epoch: 0040 loss_train: 0.3006\n",
      "Epoch: 0041 loss_train: 0.3084\n",
      "Epoch: 0042 loss_train: 0.3369\n",
      "Epoch: 0043 loss_train: 0.2876\n",
      "Epoch: 0044 loss_train: 0.2889\n",
      "Epoch: 0045 loss_train: 0.2647\n",
      "Epoch: 0046 loss_train: 0.3141\n",
      "Epoch: 0047 loss_train: 0.2928\n",
      "Epoch: 0048 loss_train: 0.2771\n",
      "Epoch: 0049 loss_train: 0.2422\n",
      "Epoch: 0050 loss_train: 0.3091\n",
      "Epoch: 0051 loss_train: 0.2616\n",
      "Epoch: 0052 loss_train: 0.2921\n",
      "Epoch: 0053 loss_train: 0.2403\n",
      "Epoch: 0054 loss_train: 0.2786\n",
      "Epoch: 0055 loss_train: 0.2564\n",
      "Epoch: 0056 loss_train: 0.2439\n",
      "Epoch: 0057 loss_train: 0.2427\n",
      "Epoch: 0058 loss_train: 0.2324\n",
      "Epoch: 0059 loss_train: 0.2283\n",
      "Epoch: 0060 loss_train: 0.2406\n",
      "Epoch: 0061 loss_train: 0.2586\n",
      "Epoch: 0062 loss_train: 0.2510\n",
      "Epoch: 0063 loss_train: 0.2339\n",
      "Epoch: 0064 loss_train: 0.2345\n",
      "Epoch: 0065 loss_train: 0.2048\n",
      "Epoch: 0066 loss_train: 0.2532\n",
      "Epoch: 0067 loss_train: 0.1979\n",
      "Epoch: 0068 loss_train: 0.2356\n",
      "Epoch: 0069 loss_train: 0.2179\n",
      "Epoch: 0070 loss_train: 0.2226\n",
      "Epoch: 0071 loss_train: 0.2001\n",
      "Epoch: 0072 loss_train: 0.1997\n",
      "Epoch: 0073 loss_train: 0.2310\n",
      "Epoch: 0074 loss_train: 0.2189\n",
      "Epoch: 0075 loss_train: 0.1966\n",
      "Epoch: 0076 loss_train: 0.2314\n",
      "Epoch: 0077 loss_train: 0.2026\n",
      "Epoch: 0078 loss_train: 0.2079\n",
      "Epoch: 0079 loss_train: 0.2150\n",
      "Epoch: 0080 loss_train: 0.2180\n",
      "Epoch: 0081 loss_train: 0.1973\n",
      "Epoch: 0082 loss_train: 0.2104\n",
      "Epoch: 0083 loss_train: 0.2200\n",
      "Epoch: 0084 loss_train: 0.1857\n",
      "Epoch: 0085 loss_train: 0.2198\n",
      "Epoch: 0086 loss_train: 0.2172\n",
      "Epoch: 0087 loss_train: 0.2206\n",
      "Epoch: 0088 loss_train: 0.2242\n",
      "Epoch: 0089 loss_train: 0.1848\n",
      "Epoch: 0090 loss_train: 0.2088\n",
      "Epoch: 0091 loss_train: 0.2106\n",
      "Epoch: 0092 loss_train: 0.1765\n",
      "Epoch: 0093 loss_train: 0.1954\n",
      "Epoch: 0094 loss_train: 0.1852\n",
      "Epoch: 0095 loss_train: 0.1716\n",
      "Epoch: 0096 loss_train: 0.1778\n",
      "Epoch: 0097 loss_train: 0.1666\n",
      "Epoch: 0098 loss_train: 0.2120\n",
      "Epoch: 0099 loss_train: 0.1778\n",
      "Epoch: 0100 loss_train: 0.2066\n",
      "Epoch: 0101 loss_train: 0.1689\n",
      "Epoch: 0102 loss_train: 0.1786\n",
      "Epoch: 0103 loss_train: 0.1955\n",
      "Epoch: 0104 loss_train: 0.1718\n",
      "Epoch: 0105 loss_train: 0.1990\n",
      "Epoch: 0106 loss_train: 0.2071\n",
      "Epoch: 0107 loss_train: 0.2126\n",
      "Epoch: 0108 loss_train: 0.2348\n",
      "Epoch: 0109 loss_train: 0.1820\n",
      "Epoch: 0110 loss_train: 0.1833\n",
      "Epoch: 0111 loss_train: 0.1819\n",
      "Epoch: 0112 loss_train: 0.1965\n",
      "Epoch: 0113 loss_train: 0.1822\n",
      "Epoch: 0114 loss_train: 0.1884\n",
      "Epoch: 0115 loss_train: 0.1533\n",
      "Epoch: 0116 loss_train: 0.1800\n",
      "Epoch: 0117 loss_train: 0.1853\n",
      "Epoch: 0118 loss_train: 0.1527\n",
      "Epoch: 0119 loss_train: 0.1795\n",
      "Epoch: 0120 loss_train: 0.1638\n",
      "Epoch: 0121 loss_train: 0.1599\n",
      "Epoch: 0122 loss_train: 0.1591\n",
      "Epoch: 0123 loss_train: 0.1792\n",
      "Epoch: 0124 loss_train: 0.1641\n",
      "Epoch: 0125 loss_train: 0.1586\n",
      "Epoch: 0126 loss_train: 0.1452\n",
      "Epoch: 0127 loss_train: 0.2130\n",
      "Epoch: 0128 loss_train: 0.1451\n",
      "Epoch: 0129 loss_train: 0.1823\n",
      "Epoch: 0130 loss_train: 0.2051\n",
      "Epoch: 0131 loss_train: 0.1593\n",
      "Epoch: 0132 loss_train: 0.1952\n",
      "Epoch: 0133 loss_train: 0.1626\n",
      "Epoch: 0134 loss_train: 0.1717\n",
      "Epoch: 0135 loss_train: 0.1859\n",
      "Epoch: 0136 loss_train: 0.1522\n",
      "Epoch: 0137 loss_train: 0.1788\n",
      "Epoch: 0138 loss_train: 0.1492\n",
      "Epoch: 0139 loss_train: 0.1628\n",
      "Epoch: 0140 loss_train: 0.1680\n",
      "Epoch: 0141 loss_train: 0.1580\n",
      "Epoch: 0142 loss_train: 0.1760\n",
      "Epoch: 0143 loss_train: 0.1656\n",
      "Epoch: 0144 loss_train: 0.1745\n",
      "Epoch: 0145 loss_train: 0.1850\n",
      "Epoch: 0146 loss_train: 0.1471\n",
      "Epoch: 0147 loss_train: 0.1781\n",
      "Epoch: 0148 loss_train: 0.1887\n",
      "Epoch: 0149 loss_train: 0.1324\n",
      "Epoch: 0150 loss_train: 0.1477\n",
      "Epoch: 0151 loss_train: 0.1638\n",
      "Epoch: 0152 loss_train: 0.1632\n",
      "Epoch: 0153 loss_train: 0.1883\n",
      "Epoch: 0154 loss_train: 0.1546\n",
      "Epoch: 0155 loss_train: 0.1532\n",
      "Epoch: 0156 loss_train: 0.1777\n",
      "Epoch: 0157 loss_train: 0.1608\n",
      "Epoch: 0158 loss_train: 0.1203\n",
      "Epoch: 0159 loss_train: 0.1513\n",
      "Epoch: 0160 loss_train: 0.1912\n",
      "Epoch: 0161 loss_train: 0.1792\n",
      "Epoch: 0162 loss_train: 0.1435\n",
      "Epoch: 0163 loss_train: 0.1785\n",
      "Epoch: 0164 loss_train: 0.1564\n",
      "Epoch: 0165 loss_train: 0.1657\n",
      "Epoch: 0166 loss_train: 0.1710\n",
      "Epoch: 0167 loss_train: 0.2075\n",
      "Epoch: 0168 loss_train: 0.1451\n",
      "Epoch: 0169 loss_train: 0.1609\n",
      "Epoch: 0170 loss_train: 0.1583\n",
      "Epoch: 0171 loss_train: 0.1574\n",
      "Epoch: 0172 loss_train: 0.1750\n",
      "Epoch: 0173 loss_train: 0.1474\n",
      "Epoch: 0174 loss_train: 0.1531\n",
      "Epoch: 0175 loss_train: 0.1655\n",
      "Epoch: 0176 loss_train: 0.1696\n",
      "Epoch: 0177 loss_train: 0.1292\n",
      "Epoch: 0178 loss_train: 0.1406\n",
      "Epoch: 0179 loss_train: 0.1410\n",
      "Epoch: 0180 loss_train: 0.1754\n",
      "Epoch: 0181 loss_train: 0.1818\n",
      "Epoch: 0182 loss_train: 0.1416\n",
      "Epoch: 0183 loss_train: 0.1404\n",
      "Epoch: 0184 loss_train: 0.1732\n",
      "Epoch: 0185 loss_train: 0.1488\n",
      "Epoch: 0186 loss_train: 0.1526\n",
      "Epoch: 0187 loss_train: 0.1678\n",
      "Epoch: 0188 loss_train: 0.1606\n",
      "Epoch: 0189 loss_train: 0.1414\n",
      "Epoch: 0190 loss_train: 0.1396\n",
      "Epoch: 0191 loss_train: 0.1562\n",
      "Epoch: 0192 loss_train: 0.1577\n",
      "Epoch: 0193 loss_train: 0.1452\n",
      "Epoch: 0194 loss_train: 0.1677\n",
      "Epoch: 0195 loss_train: 0.1784\n",
      "Epoch: 0196 loss_train: 0.1532\n",
      "Epoch: 0197 loss_train: 0.1310\n",
      "Epoch: 0198 loss_train: 0.1430\n",
      "Epoch: 0199 loss_train: 0.1571\n",
      "Epoch: 0200 loss_train: 0.1471\n",
      "Test set results: loss= 0.0282 accuracy= 0.9940\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 200 epochs and evaluate on the test set\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3af44-90b5-4dcb-8c2b-92e0bb8a7cba",
   "metadata": {},
   "source": [
    "### Doing Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7bf66fe4-9336-48b9-9e37-b737e42c2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.0889\n",
      "Epoch: 0002 loss_train: 0.1012\n",
      "Epoch: 0003 loss_train: 0.1152\n",
      "Epoch: 0004 loss_train: 0.1336\n",
      "Epoch: 0005 loss_train: 0.0936\n",
      "Epoch: 0006 loss_train: 0.1223\n",
      "Epoch: 0007 loss_train: 0.0959\n",
      "Epoch: 0008 loss_train: 0.1289\n",
      "Epoch: 0009 loss_train: 0.1428\n",
      "Epoch: 0010 loss_train: 0.0942\n",
      "Epoch: 0011 loss_train: 0.1175\n",
      "Epoch: 0012 loss_train: 0.1052\n",
      "Epoch: 0013 loss_train: 0.0932\n",
      "Epoch: 0014 loss_train: 0.1169\n",
      "Epoch: 0015 loss_train: 0.1014\n",
      "Epoch: 0016 loss_train: 0.1163\n",
      "Epoch: 0017 loss_train: 0.0914\n",
      "Epoch: 0018 loss_train: 0.1263\n",
      "Epoch: 0019 loss_train: 0.1339\n",
      "Epoch: 0020 loss_train: 0.1302\n",
      "Epoch: 0021 loss_train: 0.1005\n",
      "Epoch: 0022 loss_train: 0.0974\n",
      "Epoch: 0023 loss_train: 0.1077\n",
      "Epoch: 0024 loss_train: 0.1002\n",
      "Epoch: 0025 loss_train: 0.1143\n",
      "Epoch: 0026 loss_train: 0.1111\n",
      "Epoch: 0027 loss_train: 0.1187\n",
      "Epoch: 0028 loss_train: 0.1306\n",
      "Epoch: 0029 loss_train: 0.1466\n",
      "Epoch: 0030 loss_train: 0.1085\n",
      "Epoch: 0031 loss_train: 0.1095\n",
      "Epoch: 0032 loss_train: 0.1136\n",
      "Epoch: 0033 loss_train: 0.1299\n",
      "Epoch: 0034 loss_train: 0.1028\n",
      "Epoch: 0035 loss_train: 0.1124\n",
      "Epoch: 0036 loss_train: 0.0825\n",
      "Epoch: 0037 loss_train: 0.1219\n",
      "Epoch: 0038 loss_train: 0.1430\n",
      "Epoch: 0039 loss_train: 0.1191\n",
      "Epoch: 0040 loss_train: 0.0981\n",
      "Epoch: 0041 loss_train: 0.1520\n",
      "Epoch: 0042 loss_train: 0.1127\n",
      "Epoch: 0043 loss_train: 0.1074\n",
      "Epoch: 0044 loss_train: 0.1011\n",
      "Epoch: 0045 loss_train: 0.1354\n",
      "Epoch: 0046 loss_train: 0.1520\n",
      "Epoch: 0047 loss_train: 0.0966\n",
      "Epoch: 0048 loss_train: 0.1132\n",
      "Epoch: 0049 loss_train: 0.1078\n",
      "Epoch: 0050 loss_train: 0.1303\n",
      "Epoch: 0051 loss_train: 0.1162\n",
      "Epoch: 0052 loss_train: 0.1006\n",
      "Epoch: 0053 loss_train: 0.1153\n",
      "Epoch: 0054 loss_train: 0.1136\n",
      "Epoch: 0055 loss_train: 0.0963\n",
      "Epoch: 0056 loss_train: 0.1183\n",
      "Epoch: 0057 loss_train: 0.1353\n",
      "Epoch: 0058 loss_train: 0.1263\n",
      "Epoch: 0059 loss_train: 0.1230\n",
      "Epoch: 0060 loss_train: 0.1361\n",
      "Epoch: 0061 loss_train: 0.0896\n",
      "Epoch: 0062 loss_train: 0.1098\n",
      "Epoch: 0063 loss_train: 0.1071\n",
      "Epoch: 0064 loss_train: 0.0987\n",
      "Epoch: 0065 loss_train: 0.1202\n",
      "Epoch: 0066 loss_train: 0.1011\n",
      "Epoch: 0067 loss_train: 0.0943\n",
      "Epoch: 0068 loss_train: 0.1087\n",
      "Epoch: 0069 loss_train: 0.1309\n",
      "Epoch: 0070 loss_train: 0.1350\n",
      "Epoch: 0071 loss_train: 0.1269\n",
      "Epoch: 0072 loss_train: 0.1011\n",
      "Epoch: 0073 loss_train: 0.1448\n",
      "Epoch: 0074 loss_train: 0.0962\n",
      "Epoch: 0075 loss_train: 0.1251\n",
      "Epoch: 0076 loss_train: 0.1263\n",
      "Epoch: 0077 loss_train: 0.1068\n",
      "Epoch: 0078 loss_train: 0.1452\n",
      "Epoch: 0079 loss_train: 0.1003\n",
      "Epoch: 0080 loss_train: 0.1045\n",
      "Epoch: 0081 loss_train: 0.1289\n",
      "Epoch: 0082 loss_train: 0.1602\n",
      "Epoch: 0083 loss_train: 0.1002\n",
      "Epoch: 0084 loss_train: 0.1281\n",
      "Epoch: 0085 loss_train: 0.1208\n",
      "Epoch: 0086 loss_train: 0.1097\n",
      "Epoch: 0087 loss_train: 0.1171\n",
      "Epoch: 0088 loss_train: 0.0922\n",
      "Epoch: 0089 loss_train: 0.1006\n",
      "Epoch: 0090 loss_train: 0.1200\n",
      "Epoch: 0091 loss_train: 0.1051\n",
      "Epoch: 0092 loss_train: 0.1142\n",
      "Epoch: 0093 loss_train: 0.1065\n",
      "Epoch: 0094 loss_train: 0.1205\n",
      "Epoch: 0095 loss_train: 0.1012\n",
      "Epoch: 0096 loss_train: 0.1128\n",
      "Epoch: 0097 loss_train: 0.1052\n",
      "Epoch: 0098 loss_train: 0.1151\n",
      "Epoch: 0099 loss_train: 0.1186\n",
      "Epoch: 0100 loss_train: 0.1288\n",
      "Epoch: 0101 loss_train: 0.1150\n",
      "Epoch: 0102 loss_train: 0.1014\n",
      "Epoch: 0103 loss_train: 0.1011\n",
      "Epoch: 0104 loss_train: 0.0998\n",
      "Epoch: 0105 loss_train: 0.0897\n",
      "Epoch: 0106 loss_train: 0.1346\n",
      "Epoch: 0107 loss_train: 0.1427\n",
      "Epoch: 0108 loss_train: 0.0937\n",
      "Epoch: 0109 loss_train: 0.1008\n",
      "Epoch: 0110 loss_train: 0.1121\n",
      "Epoch: 0111 loss_train: 0.1056\n",
      "Epoch: 0112 loss_train: 0.1229\n",
      "Epoch: 0113 loss_train: 0.0964\n",
      "Epoch: 0114 loss_train: 0.1478\n",
      "Epoch: 0115 loss_train: 0.1081\n",
      "Epoch: 0116 loss_train: 0.1093\n",
      "Epoch: 0117 loss_train: 0.1200\n",
      "Epoch: 0118 loss_train: 0.1355\n",
      "Epoch: 0119 loss_train: 0.1103\n",
      "Epoch: 0120 loss_train: 0.1026\n",
      "Epoch: 0121 loss_train: 0.1227\n",
      "Epoch: 0122 loss_train: 0.1060\n",
      "Epoch: 0123 loss_train: 0.0993\n",
      "Epoch: 0124 loss_train: 0.1107\n",
      "Epoch: 0125 loss_train: 0.1121\n",
      "Epoch: 0126 loss_train: 0.1203\n",
      "Epoch: 0127 loss_train: 0.0988\n",
      "Epoch: 0128 loss_train: 0.1094\n",
      "Epoch: 0129 loss_train: 0.1152\n",
      "Epoch: 0130 loss_train: 0.1375\n",
      "Epoch: 0131 loss_train: 0.0960\n",
      "Epoch: 0132 loss_train: 0.1293\n",
      "Epoch: 0133 loss_train: 0.1067\n",
      "Epoch: 0134 loss_train: 0.1009\n",
      "Epoch: 0135 loss_train: 0.0845\n",
      "Epoch: 0136 loss_train: 0.1017\n",
      "Epoch: 0137 loss_train: 0.1040\n",
      "Epoch: 0138 loss_train: 0.1118\n",
      "Epoch: 0139 loss_train: 0.1107\n",
      "Epoch: 0140 loss_train: 0.0979\n",
      "Epoch: 0141 loss_train: 0.1107\n",
      "Epoch: 0142 loss_train: 0.0884\n",
      "Epoch: 0143 loss_train: 0.1079\n",
      "Epoch: 0144 loss_train: 0.1195\n",
      "Epoch: 0145 loss_train: 0.1168\n",
      "Epoch: 0146 loss_train: 0.1080\n",
      "Epoch: 0147 loss_train: 0.1043\n",
      "Epoch: 0148 loss_train: 0.1056\n",
      "Epoch: 0149 loss_train: 0.1239\n",
      "Epoch: 0150 loss_train: 0.0890\n",
      "Epoch: 0151 loss_train: 0.1142\n",
      "Epoch: 0152 loss_train: 0.1303\n",
      "Epoch: 0153 loss_train: 0.1352\n",
      "Epoch: 0154 loss_train: 0.1155\n",
      "Epoch: 0155 loss_train: 0.1160\n",
      "Epoch: 0156 loss_train: 0.1178\n",
      "Epoch: 0157 loss_train: 0.1122\n",
      "Epoch: 0158 loss_train: 0.1094\n",
      "Epoch: 0159 loss_train: 0.1274\n",
      "Epoch: 0160 loss_train: 0.1356\n",
      "Epoch: 0161 loss_train: 0.0925\n",
      "Epoch: 0162 loss_train: 0.1112\n",
      "Epoch: 0163 loss_train: 0.1315\n",
      "Epoch: 0164 loss_train: 0.1480\n",
      "Epoch: 0165 loss_train: 0.1255\n",
      "Epoch: 0166 loss_train: 0.0984\n",
      "Epoch: 0167 loss_train: 0.0942\n",
      "Epoch: 0168 loss_train: 0.1057\n",
      "Epoch: 0169 loss_train: 0.1117\n",
      "Epoch: 0170 loss_train: 0.1217\n",
      "Epoch: 0171 loss_train: 0.1566\n",
      "Epoch: 0172 loss_train: 0.0996\n",
      "Epoch: 0173 loss_train: 0.1239\n",
      "Epoch: 0174 loss_train: 0.1170\n",
      "Epoch: 0175 loss_train: 0.1296\n",
      "Epoch: 0176 loss_train: 0.0992\n",
      "Epoch: 0177 loss_train: 0.1042\n",
      "Epoch: 0178 loss_train: 0.0933\n",
      "Epoch: 0179 loss_train: 0.0891\n",
      "Epoch: 0180 loss_train: 0.1028\n",
      "Epoch: 0181 loss_train: 0.0863\n",
      "Epoch: 0182 loss_train: 0.1161\n",
      "Epoch: 0183 loss_train: 0.1050\n",
      "Epoch: 0184 loss_train: 0.0988\n",
      "Epoch: 0185 loss_train: 0.0999\n",
      "Epoch: 0186 loss_train: 0.1022\n",
      "Epoch: 0187 loss_train: 0.1165\n",
      "Epoch: 0188 loss_train: 0.1097\n",
      "Epoch: 0189 loss_train: 0.1059\n",
      "Epoch: 0190 loss_train: 0.1006\n",
      "Epoch: 0191 loss_train: 0.1037\n",
      "Epoch: 0192 loss_train: 0.1311\n",
      "Epoch: 0193 loss_train: 0.1182\n",
      "Epoch: 0194 loss_train: 0.1154\n",
      "Epoch: 0195 loss_train: 0.0955\n",
      "Epoch: 0196 loss_train: 0.1015\n",
      "Epoch: 0197 loss_train: 0.1229\n",
      "Epoch: 0198 loss_train: 0.1025\n",
      "Epoch: 0199 loss_train: 0.0992\n",
      "Epoch: 0200 loss_train: 0.1227\n",
      "Test set results: Loss: 0.8194 Accuracy: 0.7100 Accuracy_train: 0.9975\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not coo_matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 6. Evaluate on the test set after training\u001b[39;00m\n\u001b[0;32m     76\u001b[0m test()\n\u001b[1;32m---> 77\u001b[0m \u001b[43mtest_overall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[102], line 55\u001b[0m, in \u001b[0;36mtest_overall\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     54\u001b[0m     x   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(features_test,       dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 55\u001b[0m     adj \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_normalized_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(x, adj)\n\u001b[0;32m     58\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[idx_test_a], labels[idx_test_a])\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not coo_matrix"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Create a list of indices for all nodes\n",
    "num_nodes = features.shape[0]  # total number of nodes (e.g., 496)\n",
    "all_indices = np.arange(num_nodes)\n",
    "\n",
    "# 2. Perform an 80/20 train-test split using scikit-learn\n",
    "idx_train, idx_test = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally convert indices to a PyTorch tensor if needed:\n",
    "idx_train = torch.tensor(idx_train, dtype=torch.long)\n",
    "idx_test = torch.tensor(idx_test, dtype=torch.long)\n",
    "\n",
    "# 3. Training function (uses global idx_train)\n",
    "def train(epoch):\n",
    "    model.train()                       # Set model to training mode\n",
    "    optimizer.zero_grad()               # Clear gradients\n",
    "    output = model(features, adj_normalized)  # Forward pass\n",
    "    # Compute training loss only on the training indices\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()               # Backward pass: compute gradients\n",
    "    optimizer.step()                    # Update model parameters\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()))\n",
    "\n",
    "# 4. Testing/evaluation function (uses global idx_test)\n",
    "def test():\n",
    "    model.eval()                        # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj_normalized)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        # Compute predictions for test nodes\n",
    "        pred = output[idx_test].max(1)[1]\n",
    "        pred_train= output[idx_train].max(1)[1]\n",
    "        # Calculate accuracy on test set\n",
    "        correct = pred.eq(labels[idx_test]).sum().item()\n",
    "        accuracy = correct / len(idx_test)\n",
    "        correct_train= pred_train.eq(labels[idx_train]).sum().item()\n",
    "        accuracy_train = correct_train / len(idx_train)\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"Loss: {:.4f}\".format(loss_test.item()),\n",
    "          \"Accuracy: {:.4f}\".format(accuracy),\n",
    "         \"Accuracy_train: {:.4f}\".format(accuracy_train))\n",
    "\n",
    "def test_overall():\n",
    "    model.eval()                        # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(features_test, adj_normalized_test)\n",
    "        #print(output)\n",
    "        loss_test = F.nll_loss(output[idx_test_a], labels[idx_test_a])\n",
    "        # Compute predictions for test nodes\n",
    "        pred = output[idx_test_a].max(1)[1]\n",
    "        # Calculate accuracy on test set\n",
    "        #correct = pred.eq(labels[idx_test]).sum().item()\n",
    "        #accuracy = correct / len(idx_test)\n",
    "        #correct_train= pred_train.eq(labels[idx_train]).sum().item()\n",
    "        #accuracy_train = correct_train / len(idx_train)\n",
    "        print(pred)\n",
    "    \n",
    "    '''print(\"Test set results:\",\n",
    "          \"Loss: {:.4f}\".format(loss_test.item()),\n",
    "          \"Accuracy: {:.4f}\".format(accuracy),\n",
    "         \"Accuracy_train: {:.4f}\".format(accuracy_train))'''\n",
    "\n",
    "# 5. Training loop\n",
    "num_epochs = 200  # Adjust the number of epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "\n",
    "# 6. Evaluate on the test set after training\n",
    "test()\n",
    "test_overall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0cd2df2a-1160-44ac-9c41-2cd26eb16ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(features, adj_normalized_test, labels, idx_test_a):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features, adj_normalized_test)\n",
    "        loss = F.nll_loss(logits[idx_test_a], labels[idx_test_a]).item()\n",
    "        preds = logits[idx_test_a].max(dim=1)[1]\n",
    "        acc  = preds.eq(labels[idx_test_a]).sum().item() / idx_test_a.size(0)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "95d467be-a09a-4f09-b7d9-2b1e937df3a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mm(): argument 'input' (position 1) must be Tensor, not coo_matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_normalized_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_test_a\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[100], line 4\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(features, adj_normalized_test, labels, idx_test_a)\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_normalized_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(logits[idx_test_a], labels[idx_test_a])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      6\u001b[0m     preds \u001b[38;5;241m=\u001b[39m logits[idx_test_a]\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[94], line 10\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[93], line 24\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     22\u001b[0m support \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Perform sparse matrix multiplication with the normalized adjacency matrix\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[1;31mTypeError\u001b[0m: mm(): argument 'input' (position 1) must be Tensor, not coo_matrix"
     ]
    }
   ],
   "source": [
    "evaluate(features, adj_normalized_test, labels, idx_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c307738f-8031-447a-b0f2-2fa2031873d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mm(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_overall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 53\u001b[0m, in \u001b[0;36mtest_overall\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39meval()                        \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 53\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_normalized_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m     55\u001b[0m     loss_test \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output[idx_test_a], labels[idx_test_a])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_mining\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[44], line 22\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, adj):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Multiply input features by the weight matrix\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Perform sparse matrix multiplication with the normalized adjacency matrix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(adj, support)\n",
      "\u001b[1;31mTypeError\u001b[0m: mm(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "test_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4a4c-b36b-40d8-bcba-2a47331d7797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bca6c31d-5fbb-4f37-aab6-8b101e65cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roshn\\AppData\\Local\\Temp\\ipykernel_20396\\1639605561.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  idx = torch.tensor([coo.row, coo.col], dtype=torch.long, device=device)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2252 is out of bounds for dimension 0 with size 496",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 58\u001b[0m\n\u001b[0;32m     50\u001b[0m idx_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(idx_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 3.  Internal train/validation split (10% of labelled nodes)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     55\u001b[0m idx_tr, idx_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     56\u001b[0m     idx_train_full,\n\u001b[0;32m     57\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m,\n\u001b[1;32m---> 58\u001b[0m     stratify\u001b[38;5;241m=\u001b[39m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train_full\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     59\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m idx_tr  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(idx_tr , dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     62\u001b[0m idx_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(idx_val, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2252 is out of bounds for dimension 0 with size 496"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GCN training + prediction script\n",
    "================================\n",
    "Put this file in the same directory as:\n",
    "\n",
    "  adj.npz\n",
    "  features.npy\n",
    "  labels.npy\n",
    "  splits.json\n",
    "\n",
    "Run with:  python run_gcn.py\n",
    "\"\"\"\n",
    "\n",
    "import json, numpy as np, scipy.sparse as sp, torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0.  Helper: scipy sparse COO  ->  torch.sparse_coo_tensor\n",
    "# ----------------------------------------------------------------------\n",
    "def coo_to_torch(coo, device=None, dtype=torch.float32):\n",
    "    \"\"\"Convert a scipy.sparse.coo_matrix to a torch sparse tensor.\"\"\"\n",
    "    coo = coo.tocoo()           # make sure it *is* COO\n",
    "    idx = torch.tensor([coo.row, coo.col], dtype=torch.long, device=device)\n",
    "    val = torch.tensor(coo.data, dtype=dtype, device=device)\n",
    "    return torch.sparse_coo_tensor(idx, val, coo.shape, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Load data\n",
    "# ----------------------------------------------------------------------\n",
    "with open(\"../data/data/splits.json\") as fp:\n",
    "    spl = json.load(fp)\n",
    "\n",
    "idx_train_full = np.array(spl[\"idx_train\"])   # 496 labelled nodes&#8203;:contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}\n",
    "idx_test       = np.array(spl[\"idx_test\"])    # 1984 unlabelled nodes&#8203;:contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}\n",
    "\n",
    "features = np.load(\"../data/data/features.npy\")            # (2480, 1390)\n",
    "labels   = np.load(\"../data/data/labels.npy\")              # (496,)\n",
    "adj_coo  = sp.load_npz(\"../data/data/adj.npz\")             # sparse adjacency\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Tensors + device\n",
    "# ----------------------------------------------------------------------\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features = torch.as_tensor(features, dtype=torch.float32, device=device)\n",
    "labels   = torch.as_tensor(labels  , dtype=torch.long   , device=device)\n",
    "adj      = coo_to_torch(adj_coo, device)\n",
    "\n",
    "idx_test = torch.tensor(idx_test, dtype=torch.long, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Internal train/validation split (10% of labelled nodes)\n",
    "# ----------------------------------------------------------------------\n",
    "idx_tr, idx_val = train_test_split(\n",
    "    idx_train_full,\n",
    "    test_size=0.10,\n",
    "    stratify=labels[idx_train_full],\n",
    "    random_state=42,\n",
    ")\n",
    "idx_tr  = torch.tensor(idx_tr , dtype=torch.long, device=device)\n",
    "idx_val = torch.tensor(idx_val, dtype=torch.long, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Define the model  (same API you already had)\n",
    "# ----------------------------------------------------------------------\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_feats, out_feats) * 0.01)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(out_feats))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        out = torch.sparse.mm(adj, support)\n",
    "        return out + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConvolution(n_feat , n_hid )\n",
    "        self.gc2 = GraphConvolution(n_hid  , n_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN(features.size(1), 64, labels.max().item() + 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5.  Training loop\n",
    "# ----------------------------------------------------------------------\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(features, adj)\n",
    "    loss_tr = F.nll_loss(out[idx_tr], labels[idx_tr])\n",
    "    loss_tr.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out  = model(features, adj)[idx_val]\n",
    "            val_loss = F.nll_loss(val_out, labels[idx_val]).item()\n",
    "            val_acc  = (val_out.argmax(1) == labels[idx_val]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | train_loss {loss_tr.item():.4f} \"\n",
    "              f\"| val_loss {val_loss:.4f} | val_acc {val_acc:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6.  Predict on *unlabelled* test set and save\n",
    "# ----------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(features, adj)[idx_test].argmax(1).cpu().numpy()\n",
    "\n",
    "np.save(\"gcn_test_predictions.npy\", preds_test)\n",
    "print(f\"\\n Saved predictions for {len(idx_test)} test nodes to 'gcn_test_predictions.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5df8e39-440f-4368-9881-5483b4e83d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss 1.9627 | val_loss 2.1397 | val_acc 0.2800\n",
      "Epoch 010 | train_loss 0.8264 | val_loss 1.0054 | val_acc 0.7800\n",
      "Epoch 020 | train_loss 0.2814 | val_loss 1.0291 | val_acc 0.8000\n",
      "Epoch 030 | train_loss 0.2565 | val_loss 1.0718 | val_acc 0.8200\n",
      "Epoch 040 | train_loss 0.1394 | val_loss 0.8482 | val_acc 0.8200\n",
      "Epoch 050 | train_loss 0.0959 | val_loss 0.9690 | val_acc 0.7800\n",
      "Epoch 060 | train_loss 0.0912 | val_loss 1.0337 | val_acc 0.8200\n",
      "Epoch 070 | train_loss 0.0725 | val_loss 1.1590 | val_acc 0.7600\n",
      "Epoch 080 | train_loss 0.0533 | val_loss 1.1740 | val_acc 0.7600\n",
      "Epoch 090 | train_loss 0.0538 | val_loss 1.1502 | val_acc 0.7600\n",
      "Epoch 100 | train_loss 0.0396 | val_loss 1.1006 | val_acc 0.7800\n",
      "Epoch 110 | train_loss 0.0340 | val_loss 1.2348 | val_acc 0.7800\n",
      "Epoch 120 | train_loss 0.0470 | val_loss 1.3502 | val_acc 0.7600\n",
      "Epoch 130 | train_loss 0.0444 | val_loss 1.2822 | val_acc 0.7800\n",
      "Epoch 140 | train_loss 0.0343 | val_loss 1.4092 | val_acc 0.7800\n",
      "Epoch 150 | train_loss 0.0304 | val_loss 1.4428 | val_acc 0.7600\n",
      "Epoch 160 | train_loss 0.0390 | val_loss 1.6149 | val_acc 0.7600\n",
      "Epoch 170 | train_loss 0.0695 | val_loss 1.5725 | val_acc 0.7600\n",
      "Epoch 180 | train_loss 0.0371 | val_loss 1.5917 | val_acc 0.7200\n",
      "Epoch 190 | train_loss 0.0229 | val_loss 1.5240 | val_acc 0.7200\n",
      "Epoch 200 | train_loss 0.0234 | val_loss 1.4425 | val_acc 0.7600\n",
      "\n",
      " Predictions saved to 'gcn_test_predictions.npy'  (shape = (1984,))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run_gcn.py\n",
    "==========\n",
    "\n",
    "Train a 2layer GCN on the labelled nodes (496),\n",
    "monitor a 10% validation split, and predict the classes\n",
    "of the 1984 unlabelled test nodes.\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "gcn_test_predictions.npy   (array of shape (1984,))\n",
    "\"\"\"\n",
    "\n",
    "import json, numpy as np, scipy.sparse as sp, torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0.  Small helper: SciPy COO  ->  torch sparse tensor\n",
    "# ----------------------------------------------------------------------\n",
    "def coo_to_torch(coo, device=None, dtype=torch.float32):\n",
    "    \"\"\"Convert scipy.sparse.coo_matrix -> torch.sparse_coo_tensor\"\"\"\n",
    "    coo = coo.tocoo()\n",
    "    idx = torch.tensor([coo.row, coo.col], dtype=torch.long, device=device)\n",
    "    val = torch.tensor(coo.data, dtype=dtype, device=device)\n",
    "    return torch.sparse_coo_tensor(idx, val, coo.shape, device=device)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Load data\n",
    "# ----------------------------------------------------------------------\n",
    "with open(\"../data/data/splits.json\") as fp:\n",
    "    splits = json.load(fp)\n",
    "\n",
    "idx_labelled = np.array(splits[\"idx_train\"])   # 496 labelled nodeIDs\n",
    "idx_test     = np.array(splits[\"idx_test\"])    # 1984 unlabelled nodeIDs\n",
    "\n",
    "features = np.load(\"../data/data/features.npy\")             # (2480, 1390)\n",
    "raw_labels = np.load(\"../data/data/labels.npy\")             # (496,)  aligned with idx_labelled\n",
    "adj_coo   = sp.load_npz(\"../data/data/adj.npz\")             # sparse adjacency\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Build fulllength label vector  (1 = unknown)\n",
    "# ----------------------------------------------------------------------\n",
    "num_nodes  = features.shape[0]\n",
    "labels_all = -torch.ones(num_nodes, dtype=torch.long)  # starts with -1\n",
    "labels_all[idx_labelled] = torch.as_tensor(raw_labels, dtype=torch.long)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Device + tensor conversions\n",
    "# ----------------------------------------------------------------------\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features = torch.as_tensor(features, dtype=torch.float32, device=device)\n",
    "labels_all = labels_all.to(device)\n",
    "adj = coo_to_torch(adj_coo, device)\n",
    "\n",
    "idx_test = torch.tensor(idx_test, dtype=torch.long, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Internal 90%/10% split of the *labelled* nodes\n",
    "# ----------------------------------------------------------------------\n",
    "pos = np.arange(len(idx_labelled))                     # 0  495\n",
    "pos_tr, pos_val = train_test_split(\n",
    "        pos,\n",
    "        test_size=0.10,\n",
    "        stratify=raw_labels,\n",
    "        random_state=42,\n",
    ")\n",
    "\n",
    "idx_tr  = torch.as_tensor(idx_labelled[pos_tr] , dtype=torch.long, device=device)\n",
    "idx_val = torch.as_tensor(idx_labelled[pos_val], dtype=torch.long, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5.  GCN definition\n",
    "# ----------------------------------------------------------------------\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_feats, out_feats) * 0.01)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(out_feats))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        out = torch.sparse.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConvolution(n_feat, n_hid)\n",
    "        self.gc2 = GraphConvolution(n_hid, n_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = GCN(features.size(1), 64, int(raw_labels.max()) + 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6.  Training loop\n",
    "# ----------------------------------------------------------------------\n",
    "for epoch in range(1, 201):\n",
    "    # --- training step ---\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(features, adj)\n",
    "    loss_tr = F.nll_loss(logits[idx_tr], labels_all[idx_tr])\n",
    "    loss_tr.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- validation step ---\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(features, adj)[idx_val]\n",
    "            val_loss = F.nll_loss(val_logits, labels_all[idx_val]).item()\n",
    "            val_acc = (val_logits.argmax(1) == labels_all[idx_val]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | train_loss {loss_tr.item():.4f} \"\n",
    "              f\"| val_loss {val_loss:.4f} | val_acc {val_acc:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7.  Predict on the unlabelled 1984 test nodes\n",
    "# ----------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(features, adj)[idx_test].argmax(1).cpu().numpy()\n",
    "\n",
    "np.save(\"gcn_test_predictions.npy\", preds_test)\n",
    "print(f\"\\n Predictions saved to 'gcn_test_predictions.npy'  \"\n",
    "      f\"(shape = {preds_test.shape})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "249d4088-cbf1-4a38-9a7d-f5d84da54c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote 1984 rows to 'test_predictions.csv'\n",
      "   node_id  predicted_label\n",
      "0        0                5\n",
      "1        4                2\n",
      "2        7                6\n",
      "3       10                5\n",
      "4       11                6\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "\n",
    "# 1) read the testnode IDs (1984 integers) -----------------------------\n",
    "idx_test = np.array(json.load(open(\"../data/data/splits.json\"))[\"idx_test\"])\n",
    "\n",
    "# 2) read the predictions you just saved --------------------------------\n",
    "preds = np.load(\"gcn_test_predictions.npy\")     # shape = (1984,)\n",
    "\n",
    "assert len(idx_test) == len(preds), \"ID / prediction length mismatch\"\n",
    "\n",
    "# 3) make a table and save it -------------------------------------------\n",
    "df = pd.DataFrame(\n",
    "        {\"node_id\": idx_test,\n",
    "         \"predicted_label\": preds.astype(int)}\n",
    ")\n",
    "# Sort by node_id so the CSV is in natural order (optional)\n",
    "df = df.sort_values(\"node_id\", ignore_index=True)\n",
    "\n",
    "df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\" Wrote\", len(df), \"rows to 'test_predictions.csv'\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b096d6a9-6176-442a-a0f9-035acaf91524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2475</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>2478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2479</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_id  predicted_label\n",
       "0           0                5\n",
       "1           4                2\n",
       "2           7                6\n",
       "3          10                5\n",
       "4          11                6\n",
       "...       ...              ...\n",
       "1979     2475                6\n",
       "1980     2476                1\n",
       "1981     2477                1\n",
       "1982     2478                2\n",
       "1983     2479                6\n",
       "\n",
       "[1984 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a150dc-0722-43ed-af53-8383d84c2cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e14f139e-6d56-42dd-9eb2-795b6704bf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_loss 1.9459 | val_loss 1.9322 | val_acc 0.2800\n",
      "Epoch 010 | tr_loss 1.4794 | val_loss 1.4781 | val_acc 0.3400\n",
      "Epoch 020 | tr_loss 0.7835 | val_loss 0.9474 | val_acc 0.7400\n",
      "Epoch 030 | tr_loss 0.3994 | val_loss 0.6705 | val_acc 0.7600\n",
      "Epoch 040 | tr_loss 0.2413 | val_loss 0.5655 | val_acc 0.8000\n",
      "Epoch 050 | tr_loss 0.1678 | val_loss 0.5126 | val_acc 0.8200\n",
      "Epoch 060 | tr_loss 0.1371 | val_loss 0.5155 | val_acc 0.8000\n",
      "Epoch 070 | tr_loss 0.1305 | val_loss 0.5130 | val_acc 0.8200\n",
      "Epoch 080 | tr_loss 0.1124 | val_loss 0.5297 | val_acc 0.8200\n",
      "Epoch 090 | tr_loss 0.1069 | val_loss 0.5214 | val_acc 0.8400\n",
      "> Earlystopped at epoch 94  (best epoch = 74)\n",
      "Training time: 8.6s | best_val_loss = 0.5012\n",
      "\n",
      " Saved predictions for 1984 nodes to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run_gcn_plus.py\n",
    "===============\n",
    "\n",
    "Improved GCN with selfloops, feature normalisation, wider hidden layer,\n",
    "lower dropout and earlystopping.  Produces:\n",
    "\n",
    "    test_predictions.csv   (node_id, predicted_label)\n",
    "\"\"\"\n",
    "\n",
    "import json, numpy as np, scipy.sparse as sp, torch, time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 0.  Helper functions\n",
    "# --------------------------------------------------------------------- #\n",
    "def coo_to_torch(coo, device=None, dtype=torch.float32):\n",
    "    coo = coo.tocoo()\n",
    "    idx = torch.tensor([coo.row, coo.col], dtype=torch.long, device=device)\n",
    "    val = torch.tensor(coo.data, dtype=dtype, device=device)\n",
    "    return torch.sparse_coo_tensor(idx, val, coo.shape, device=device)\n",
    "\n",
    "def normalize_adj(coo):\n",
    "    \"\"\"  = D^ (A + I) D^   (symmetric) \"\"\"\n",
    "    coo = (coo + sp.eye(coo.shape[0], dtype=coo.dtype)).tocoo()\n",
    "    deg = np.asarray(coo.sum(1)).flatten()\n",
    "    deg_inv_sqrt = np.power(deg, -0.5);  deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0\n",
    "    D_inv = sp.diags(deg_inv_sqrt)\n",
    "    return D_inv @ coo @ D_inv      # still sparse\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 1.  Load data\n",
    "# --------------------------------------------------------------------- #\n",
    "with open(\"../data/data/splits.json\") as fp:\n",
    "    splits = json.load(fp)\n",
    "\n",
    "idx_labelled = np.array(splits[\"idx_train\"])          # 496 labelled nodes\n",
    "idx_test     = np.array(splits[\"idx_test\"])           # 1984 unlabelled\n",
    "\n",
    "features = np.load(\"../data/data/features.npy\")                    # (2480, 1390)\n",
    "raw_labels = np.load(\"../data/data/labels.npy\")                    # (496,)\n",
    "adj_raw   = sp.load_npz(\"../data/data/adj.npz\")\n",
    "\n",
    "num_nodes = features.shape[0]\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 2.  Preprocessing\n",
    "# --------------------------------------------------------------------- #\n",
    "# -- adjacency --\n",
    "adj_norm = normalize_adj(adj_raw)                     # +I and symmetricnorm\n",
    "\n",
    "# -- feature row L2 norm --\n",
    "row_norm = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "row_norm[row_norm == 0] = 1\n",
    "features = features / row_norm\n",
    "\n",
    "# -- full label vector (1 = unknown) --\n",
    "labels_all = -torch.ones(num_nodes, dtype=torch.long)\n",
    "labels_all[idx_labelled] = torch.as_tensor(raw_labels, dtype=torch.long)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 3.  Torch tensors + device\n",
    "# --------------------------------------------------------------------- #\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features = torch.as_tensor(features, dtype=torch.float32, device=device)\n",
    "labels_all = labels_all.to(device)\n",
    "adj = coo_to_torch(adj_norm, device)\n",
    "\n",
    "idx_test = torch.tensor(idx_test, dtype=torch.long, device=device)\n",
    "\n",
    "# train/val split inside the 496 nodes\n",
    "pos = np.arange(len(idx_labelled))\n",
    "pos_tr, pos_val = train_test_split(\n",
    "        pos, test_size=0.10, stratify=raw_labels, random_state=42)\n",
    "idx_tr  = torch.as_tensor(idx_labelled[pos_tr] , dtype=torch.long, device=device)\n",
    "idx_val = torch.as_tensor(idx_labelled[pos_val], dtype=torch.long, device=device)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 4.  Model\n",
    "# --------------------------------------------------------------------- #\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(in_feats, out_feats) * 0.01)\n",
    "        self.bias   = torch.nn.Parameter(torch.zeros(out_feats)) if bias else None\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        out = torch.sparse.mm(adj, support)\n",
    "        return out + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConvolution(n_feat, n_hid)\n",
    "        self.gc2 = GraphConvolution(n_hid, n_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN(features.size(1), 128, int(raw_labels.max())+1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 5.  Training with earlystopping\n",
    "# --------------------------------------------------------------------- #\n",
    "best_val   = float(\"inf\")\n",
    "best_epoch = 0\n",
    "patience   = 20\n",
    "start      = time.time()\n",
    "\n",
    "for epoch in range(1, 501):                      # up to 500 epochs\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    out = model(features, adj)\n",
    "    loss_tr = F.nll_loss(out[idx_tr], labels_all[idx_tr])\n",
    "    loss_tr.backward();  optim.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_val  = model(features, adj)[idx_val]\n",
    "        loss_val = F.nll_loss(out_val, labels_all[idx_val]).item()\n",
    "        acc_val  = (out_val.argmax(1) == labels_all[idx_val]).float().mean().item()\n",
    "\n",
    "    if loss_val < best_val - 1e-4:               # improvement margin\n",
    "        best_val, best_epoch = loss_val, epoch\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | tr_loss {loss_tr.item():.4f} \"\n",
    "              f\"| val_loss {loss_val:.4f} | val_acc {acc_val:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"> Earlystopped at epoch {epoch}  (best epoch = {best_epoch})\")\n",
    "        break\n",
    "\n",
    "print(f\"Training time: {time.time()-start:.1f}s | best_val_loss = {best_val:.4f}\")\n",
    "\n",
    "# restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 6.  Predict on the test nodes & save CSV\n",
    "# --------------------------------------------------------------------- #\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(features, adj)[idx_test].argmax(1).cpu().numpy()\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({\"node_id\": idx_test.cpu().numpy(),\n",
    "              \"predicted_label\": preds}) \\\n",
    "  .sort_values(\"node_id\") \\\n",
    "  .to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"\\n Saved predictions for {len(preds)} nodes to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03074241-162c-468b-a9e2-9a9862a5bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wrote test_predictions_original_order.csv  (rows match idx_test order)\n",
      " wrote gcn_test_predictions.npy             (shape = (1984,) )\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "\n",
    "# 1  read the 1984 testnode IDs in their original order\n",
    "idx_test = np.array(json.load(open(\"../data/data/splits.json\"))[\"idx_test\"])\n",
    "\n",
    "# 2  read the (sorted) predictions we just produced\n",
    "pred_df = pd.read_csv(\"test_predictions.csv\")          # 2 columns\n",
    "\n",
    "# 3  map node_id -> label and reorder\n",
    "pred_dict = dict(zip(pred_df.node_id, pred_df.predicted_label))\n",
    "preds_ordered = np.array([pred_dict[node] for node in idx_test], dtype=int)\n",
    "\n",
    "# 4  save BOTH formats\n",
    "pd.DataFrame({\"node_id\": idx_test,\n",
    "              \"predicted_label\": preds_ordered}).to_csv(\n",
    "                  \"test_predictions_original_order.csv\", index=False)\n",
    "\n",
    "np.save(\"gcn_test_predictions.npy\", preds_ordered)\n",
    "\n",
    "print(\" wrote test_predictions_original_order.csv  (rows match idx_test order)\")\n",
    "print(\" wrote gcn_test_predictions.npy             (shape =\", preds_ordered.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e771af5-144a-4192-aad8-5d681f53e85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2475</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>2478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2479</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_id  predicted_label\n",
       "0           0                5\n",
       "1           4                2\n",
       "2           7                6\n",
       "3          10                5\n",
       "4          11                6\n",
       "...       ...              ...\n",
       "1979     2475                6\n",
       "1980     2476                1\n",
       "1981     2477                1\n",
       "1982     2478                2\n",
       "1983     2479                6\n",
       "\n",
       "[1984 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1ea2130b-ef02-43b3-9f44-52f7fc2288a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wrote test_predictions.txt  (tabseparated)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pred_df = pd.read_csv(\"test_predictions.csv\")   # the file you already have\n",
    "\n",
    "#  1. tabseparated text  (common choice) \n",
    "pred_df['predicted_label'].to_csv(\"test_predictions.txt\",\n",
    "               sep=\"\\t\",        # tab character\n",
    "               index=False,     # no row numbers\n",
    "               header=True)     # keep column names\n",
    "print(\" wrote test_predictions.txt  (tabseparated)\")\n",
    "\n",
    "#  2. spaceseparated text (if you prefer) \n",
    "# pred_df.to_csv(\"test_predictions.txt\", sep=\" \", index=False, header=False)\n",
    "\n",
    "#  3. plain fixedwidth table (no delimiter, humanreadable) \n",
    "# with open(\"test_predictions.txt\", \"w\") as f:\n",
    "#     f.write(pred_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217097c-6b5f-4d14-8f88-6a2061c92f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
