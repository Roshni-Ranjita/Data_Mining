{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31551102-0d1d-4782-94a1-af4860fe322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2368b174-c2ee-4fc8-b547-e5bdc0e6ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "adj = sp.load_npz(\"../data/data/adj.npz\")\n",
    "features = np.load(\"../data/data/features.npy\")\n",
    "labels = np.load(\"../data/data/labels.npy\")\n",
    "with open(\"../data/data/splits.json\") as f:\n",
    "    splits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad18d84-7c57-4c6d-902e-6e650bbdcd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (2480, 2480)\n",
      "Features shape: (2480, 1390)\n",
      "Labels shape: (496,)\n",
      "Train set size: 496, Test set size: 1984\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adjacency matrix shape: {adj.shape}\")\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Train set size: {len(splits['idx_train'])}, Test set size: {len(splits['idx_test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed40c75-f651-476e-9540-e02545f9e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy array (in case it's a list)\n",
    "idx_train = np.array(splits['idx_train'])\n",
    "idx_test = np.array(splits['idx_test'])\n",
    "\n",
    "# Extract training features\n",
    "X_train_features = pd.DataFrame(features[idx_train]).reset_index(drop=True)\n",
    "X_test_features = pd.DataFrame(features[idx_test]).reset_index(drop=True)\n",
    "\n",
    "X_train_adj = pd.DataFrame(adj[idx_train]).reset_index(drop=True)\n",
    "X_test_adj = pd.DataFrame(adj[idx_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be1601-334b-4f7e-bb96-0b7721972da0",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66da277-270d-4f54-98f3-c11dedf14eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec486e02-d807-49fc-b5ad-4a75f7f5cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adjacency matrix (assumed stored as a sparse matrix)\n",
    "adj = sp.load_npz('../data/data/adj.npz')[idx_train, :][:, idx_train]\n",
    "\n",
    "\n",
    "# Load node features and labels (assumed to be in numpy array format)\n",
    "features = np.load('../data/data/features.npy')[idx_train]\n",
    "labels = np.load('../data/data/labels.npy')\n",
    "\n",
    "# Load training and testing splits from splits.json\n",
    "with open('../data/data/splits.json') as f:\n",
    "    splits = json.load(f)\n",
    "idx_train = splits['idx_train']\n",
    "idx_test = splits['idx_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf168e73-94ed-4946-a2c2-3b1eed6cdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Add self-loops to the adjacency matrix and normalize it.\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize the adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    # Add self-loops\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    # Handle inf entries (in case of isolated nodes)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "adj_normalized = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333fc4ed-4765-4f8b-bac9-e9baac3f514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the normalized sparse matrix to a torch sparse tensor.\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "adj_normalized = sparse_mx_to_torch_sparse_tensor(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcc02838-1ad4-4cb4-bfd0-bfc979660fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels to torch tensors\n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73729576-d054-418c-8d0d-751ef24d02da",
   "metadata": {},
   "source": [
    "### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fcdd501-3956-45bb-89b9-80033d65bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution layer as described in Kipf & Welling (2017)\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        # Multiply input features by the weight matrix\n",
    "        support = torch.mm(input, self.weight)\n",
    "        # Perform sparse matrix multiplication with the normalized adjacency matrix\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ecc7559-b8fb-4c42-8385-41dd9bb8fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cffcd7ba-c6bf-41b7-8c40-40aa24ae9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afe855e9-c3cb-492c-a672-567bb116d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj_normalized)\n",
    "    loss_train = F.nll_loss(output, labels)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d566bd8-827f-4cbc-8bc6-0fc3b34b7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj_normalized)\n",
    "    loss_test = F.nll_loss(output, labels)\n",
    "    preds = output.max(1)[1]\n",
    "    correct = preds.eq(labels).sum().item()\n",
    "    acc_test = correct / len(idx_train)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd317b3f-34a8-4de3-85b0-b769eb7ca848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.1231\n",
      "Epoch: 0002 loss_train: 0.1576\n",
      "Epoch: 0003 loss_train: 0.1181\n",
      "Epoch: 0004 loss_train: 0.1263\n",
      "Epoch: 0005 loss_train: 0.1335\n",
      "Epoch: 0006 loss_train: 0.1238\n",
      "Epoch: 0007 loss_train: 0.1285\n",
      "Epoch: 0008 loss_train: 0.1308\n",
      "Epoch: 0009 loss_train: 0.1309\n",
      "Epoch: 0010 loss_train: 0.1237\n",
      "Epoch: 0011 loss_train: 0.1307\n",
      "Epoch: 0012 loss_train: 0.1089\n",
      "Epoch: 0013 loss_train: 0.1342\n",
      "Epoch: 0014 loss_train: 0.1642\n",
      "Epoch: 0015 loss_train: 0.1455\n",
      "Epoch: 0016 loss_train: 0.1257\n",
      "Epoch: 0017 loss_train: 0.1231\n",
      "Epoch: 0018 loss_train: 0.1224\n",
      "Epoch: 0019 loss_train: 0.1307\n",
      "Epoch: 0020 loss_train: 0.1293\n",
      "Epoch: 0021 loss_train: 0.1397\n",
      "Epoch: 0022 loss_train: 0.1266\n",
      "Epoch: 0023 loss_train: 0.1280\n",
      "Epoch: 0024 loss_train: 0.1167\n",
      "Epoch: 0025 loss_train: 0.1083\n",
      "Epoch: 0026 loss_train: 0.1231\n",
      "Epoch: 0027 loss_train: 0.1410\n",
      "Epoch: 0028 loss_train: 0.1502\n",
      "Epoch: 0029 loss_train: 0.1332\n",
      "Epoch: 0030 loss_train: 0.1286\n",
      "Epoch: 0031 loss_train: 0.1380\n",
      "Epoch: 0032 loss_train: 0.1470\n",
      "Epoch: 0033 loss_train: 0.1535\n",
      "Epoch: 0034 loss_train: 0.1375\n",
      "Epoch: 0035 loss_train: 0.1387\n",
      "Epoch: 0036 loss_train: 0.1306\n",
      "Epoch: 0037 loss_train: 0.1216\n",
      "Epoch: 0038 loss_train: 0.1437\n",
      "Epoch: 0039 loss_train: 0.1442\n",
      "Epoch: 0040 loss_train: 0.1468\n",
      "Epoch: 0041 loss_train: 0.1523\n",
      "Epoch: 0042 loss_train: 0.1488\n",
      "Epoch: 0043 loss_train: 0.1601\n",
      "Epoch: 0044 loss_train: 0.1424\n",
      "Epoch: 0045 loss_train: 0.1556\n",
      "Epoch: 0046 loss_train: 0.1434\n",
      "Epoch: 0047 loss_train: 0.1263\n",
      "Epoch: 0048 loss_train: 0.1241\n",
      "Epoch: 0049 loss_train: 0.1391\n",
      "Epoch: 0050 loss_train: 0.1243\n",
      "Epoch: 0051 loss_train: 0.1309\n",
      "Epoch: 0052 loss_train: 0.1145\n",
      "Epoch: 0053 loss_train: 0.1445\n",
      "Epoch: 0054 loss_train: 0.1127\n",
      "Epoch: 0055 loss_train: 0.1277\n",
      "Epoch: 0056 loss_train: 0.1234\n",
      "Epoch: 0057 loss_train: 0.1288\n",
      "Epoch: 0058 loss_train: 0.1138\n",
      "Epoch: 0059 loss_train: 0.1138\n",
      "Epoch: 0060 loss_train: 0.1430\n",
      "Epoch: 0061 loss_train: 0.1490\n",
      "Epoch: 0062 loss_train: 0.1232\n",
      "Epoch: 0063 loss_train: 0.1160\n",
      "Epoch: 0064 loss_train: 0.1259\n",
      "Epoch: 0065 loss_train: 0.1226\n",
      "Epoch: 0066 loss_train: 0.1263\n",
      "Epoch: 0067 loss_train: 0.1721\n",
      "Epoch: 0068 loss_train: 0.1406\n",
      "Epoch: 0069 loss_train: 0.1129\n",
      "Epoch: 0070 loss_train: 0.1441\n",
      "Epoch: 0071 loss_train: 0.1143\n",
      "Epoch: 0072 loss_train: 0.1457\n",
      "Epoch: 0073 loss_train: 0.1216\n",
      "Epoch: 0074 loss_train: 0.1487\n",
      "Epoch: 0075 loss_train: 0.1480\n",
      "Epoch: 0076 loss_train: 0.1376\n",
      "Epoch: 0077 loss_train: 0.1199\n",
      "Epoch: 0078 loss_train: 0.1405\n",
      "Epoch: 0079 loss_train: 0.1252\n",
      "Epoch: 0080 loss_train: 0.1450\n",
      "Epoch: 0081 loss_train: 0.1200\n",
      "Epoch: 0082 loss_train: 0.1521\n",
      "Epoch: 0083 loss_train: 0.1225\n",
      "Epoch: 0084 loss_train: 0.1206\n",
      "Epoch: 0085 loss_train: 0.1110\n",
      "Epoch: 0086 loss_train: 0.1349\n",
      "Epoch: 0087 loss_train: 0.1405\n",
      "Epoch: 0088 loss_train: 0.1520\n",
      "Epoch: 0089 loss_train: 0.1343\n",
      "Epoch: 0090 loss_train: 0.1138\n",
      "Epoch: 0091 loss_train: 0.1047\n",
      "Epoch: 0092 loss_train: 0.1494\n",
      "Epoch: 0093 loss_train: 0.1542\n",
      "Epoch: 0094 loss_train: 0.1163\n",
      "Epoch: 0095 loss_train: 0.1457\n",
      "Epoch: 0096 loss_train: 0.1297\n",
      "Epoch: 0097 loss_train: 0.1314\n",
      "Epoch: 0098 loss_train: 0.1400\n",
      "Epoch: 0099 loss_train: 0.1326\n",
      "Epoch: 0100 loss_train: 0.1583\n",
      "Epoch: 0101 loss_train: 0.1180\n",
      "Epoch: 0102 loss_train: 0.1431\n",
      "Epoch: 0103 loss_train: 0.1556\n",
      "Epoch: 0104 loss_train: 0.1588\n",
      "Epoch: 0105 loss_train: 0.1189\n",
      "Epoch: 0106 loss_train: 0.0943\n",
      "Epoch: 0107 loss_train: 0.1383\n",
      "Epoch: 0108 loss_train: 0.1340\n",
      "Epoch: 0109 loss_train: 0.1214\n",
      "Epoch: 0110 loss_train: 0.1347\n",
      "Epoch: 0111 loss_train: 0.1300\n",
      "Epoch: 0112 loss_train: 0.1121\n",
      "Epoch: 0113 loss_train: 0.1274\n",
      "Epoch: 0114 loss_train: 0.1073\n",
      "Epoch: 0115 loss_train: 0.1294\n",
      "Epoch: 0116 loss_train: 0.1327\n",
      "Epoch: 0117 loss_train: 0.1241\n",
      "Epoch: 0118 loss_train: 0.1208\n",
      "Epoch: 0119 loss_train: 0.1179\n",
      "Epoch: 0120 loss_train: 0.1240\n",
      "Epoch: 0121 loss_train: 0.1208\n",
      "Epoch: 0122 loss_train: 0.1193\n",
      "Epoch: 0123 loss_train: 0.1215\n",
      "Epoch: 0124 loss_train: 0.1139\n",
      "Epoch: 0125 loss_train: 0.1363\n",
      "Epoch: 0126 loss_train: 0.1229\n",
      "Epoch: 0127 loss_train: 0.1325\n",
      "Epoch: 0128 loss_train: 0.1217\n",
      "Epoch: 0129 loss_train: 0.1358\n",
      "Epoch: 0130 loss_train: 0.1371\n",
      "Epoch: 0131 loss_train: 0.1425\n",
      "Epoch: 0132 loss_train: 0.1253\n",
      "Epoch: 0133 loss_train: 0.1166\n",
      "Epoch: 0134 loss_train: 0.1516\n",
      "Epoch: 0135 loss_train: 0.1353\n",
      "Epoch: 0136 loss_train: 0.1349\n",
      "Epoch: 0137 loss_train: 0.1280\n",
      "Epoch: 0138 loss_train: 0.1151\n",
      "Epoch: 0139 loss_train: 0.1515\n",
      "Epoch: 0140 loss_train: 0.1287\n",
      "Epoch: 0141 loss_train: 0.1404\n",
      "Epoch: 0142 loss_train: 0.1325\n",
      "Epoch: 0143 loss_train: 0.1324\n",
      "Epoch: 0144 loss_train: 0.1181\n",
      "Epoch: 0145 loss_train: 0.1424\n",
      "Epoch: 0146 loss_train: 0.1427\n",
      "Epoch: 0147 loss_train: 0.1107\n",
      "Epoch: 0148 loss_train: 0.1497\n",
      "Epoch: 0149 loss_train: 0.1283\n",
      "Epoch: 0150 loss_train: 0.1562\n",
      "Epoch: 0151 loss_train: 0.1303\n",
      "Epoch: 0152 loss_train: 0.1402\n",
      "Epoch: 0153 loss_train: 0.1373\n",
      "Epoch: 0154 loss_train: 0.1119\n",
      "Epoch: 0155 loss_train: 0.1276\n",
      "Epoch: 0156 loss_train: 0.1139\n",
      "Epoch: 0157 loss_train: 0.1345\n",
      "Epoch: 0158 loss_train: 0.1238\n",
      "Epoch: 0159 loss_train: 0.1184\n",
      "Epoch: 0160 loss_train: 0.1406\n",
      "Epoch: 0161 loss_train: 0.1274\n",
      "Epoch: 0162 loss_train: 0.1285\n",
      "Epoch: 0163 loss_train: 0.1188\n",
      "Epoch: 0164 loss_train: 0.1293\n",
      "Epoch: 0165 loss_train: 0.0988\n",
      "Epoch: 0166 loss_train: 0.1218\n",
      "Epoch: 0167 loss_train: 0.1219\n",
      "Epoch: 0168 loss_train: 0.1337\n",
      "Epoch: 0169 loss_train: 0.1244\n",
      "Epoch: 0170 loss_train: 0.1222\n",
      "Epoch: 0171 loss_train: 0.1315\n",
      "Epoch: 0172 loss_train: 0.1102\n",
      "Epoch: 0173 loss_train: 0.1103\n",
      "Epoch: 0174 loss_train: 0.1200\n",
      "Epoch: 0175 loss_train: 0.1411\n",
      "Epoch: 0176 loss_train: 0.1394\n",
      "Epoch: 0177 loss_train: 0.1147\n",
      "Epoch: 0178 loss_train: 0.1167\n",
      "Epoch: 0179 loss_train: 0.1440\n",
      "Epoch: 0180 loss_train: 0.1359\n",
      "Epoch: 0181 loss_train: 0.1131\n",
      "Epoch: 0182 loss_train: 0.1196\n",
      "Epoch: 0183 loss_train: 0.1434\n",
      "Epoch: 0184 loss_train: 0.1214\n",
      "Epoch: 0185 loss_train: 0.1299\n",
      "Epoch: 0186 loss_train: 0.1505\n",
      "Epoch: 0187 loss_train: 0.1290\n",
      "Epoch: 0188 loss_train: 0.1171\n",
      "Epoch: 0189 loss_train: 0.1707\n",
      "Epoch: 0190 loss_train: 0.1259\n",
      "Epoch: 0191 loss_train: 0.1371\n",
      "Epoch: 0192 loss_train: 0.1164\n",
      "Epoch: 0193 loss_train: 0.1152\n",
      "Epoch: 0194 loss_train: 0.1495\n",
      "Epoch: 0195 loss_train: 0.1255\n",
      "Epoch: 0196 loss_train: 0.1619\n",
      "Epoch: 0197 loss_train: 0.1365\n",
      "Epoch: 0198 loss_train: 0.1351\n",
      "Epoch: 0199 loss_train: 0.1717\n",
      "Epoch: 0200 loss_train: 0.1274\n",
      "Test set results: loss= 0.0206 accuracy= 0.9919\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 200 epochs and evaluate on the test set\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bf66fe4-9336-48b9-9e37-b737e42c2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.0931\n",
      "Epoch: 0002 loss_train: 0.1124\n",
      "Epoch: 0003 loss_train: 0.1205\n",
      "Epoch: 0004 loss_train: 0.1111\n",
      "Epoch: 0005 loss_train: 0.1252\n",
      "Epoch: 0006 loss_train: 0.1277\n",
      "Epoch: 0007 loss_train: 0.1142\n",
      "Epoch: 0008 loss_train: 0.1261\n",
      "Epoch: 0009 loss_train: 0.0958\n",
      "Epoch: 0010 loss_train: 0.1056\n",
      "Epoch: 0011 loss_train: 0.1379\n",
      "Epoch: 0012 loss_train: 0.1020\n",
      "Epoch: 0013 loss_train: 0.0941\n",
      "Epoch: 0014 loss_train: 0.0985\n",
      "Epoch: 0015 loss_train: 0.1196\n",
      "Epoch: 0016 loss_train: 0.1026\n",
      "Epoch: 0017 loss_train: 0.0860\n",
      "Epoch: 0018 loss_train: 0.1006\n",
      "Epoch: 0019 loss_train: 0.1201\n",
      "Epoch: 0020 loss_train: 0.1183\n",
      "Epoch: 0021 loss_train: 0.1141\n",
      "Epoch: 0022 loss_train: 0.1028\n",
      "Epoch: 0023 loss_train: 0.1026\n",
      "Epoch: 0024 loss_train: 0.0927\n",
      "Epoch: 0025 loss_train: 0.1169\n",
      "Epoch: 0026 loss_train: 0.0990\n",
      "Epoch: 0027 loss_train: 0.1384\n",
      "Epoch: 0028 loss_train: 0.1252\n",
      "Epoch: 0029 loss_train: 0.1205\n",
      "Epoch: 0030 loss_train: 0.1143\n",
      "Epoch: 0031 loss_train: 0.1167\n",
      "Epoch: 0032 loss_train: 0.1006\n",
      "Epoch: 0033 loss_train: 0.1020\n",
      "Epoch: 0034 loss_train: 0.1042\n",
      "Epoch: 0035 loss_train: 0.1180\n",
      "Epoch: 0036 loss_train: 0.1102\n",
      "Epoch: 0037 loss_train: 0.0939\n",
      "Epoch: 0038 loss_train: 0.1343\n",
      "Epoch: 0039 loss_train: 0.1011\n",
      "Epoch: 0040 loss_train: 0.1159\n",
      "Epoch: 0041 loss_train: 0.1282\n",
      "Epoch: 0042 loss_train: 0.1032\n",
      "Epoch: 0043 loss_train: 0.1020\n",
      "Epoch: 0044 loss_train: 0.1016\n",
      "Epoch: 0045 loss_train: 0.0959\n",
      "Epoch: 0046 loss_train: 0.1107\n",
      "Epoch: 0047 loss_train: 0.0986\n",
      "Epoch: 0048 loss_train: 0.1139\n",
      "Epoch: 0049 loss_train: 0.0861\n",
      "Epoch: 0050 loss_train: 0.1000\n",
      "Epoch: 0051 loss_train: 0.0945\n",
      "Epoch: 0052 loss_train: 0.0941\n",
      "Epoch: 0053 loss_train: 0.1140\n",
      "Epoch: 0054 loss_train: 0.0866\n",
      "Epoch: 0055 loss_train: 0.1104\n",
      "Epoch: 0056 loss_train: 0.0776\n",
      "Epoch: 0057 loss_train: 0.0977\n",
      "Epoch: 0058 loss_train: 0.1264\n",
      "Epoch: 0059 loss_train: 0.1021\n",
      "Epoch: 0060 loss_train: 0.0976\n",
      "Epoch: 0061 loss_train: 0.1163\n",
      "Epoch: 0062 loss_train: 0.1040\n",
      "Epoch: 0063 loss_train: 0.1423\n",
      "Epoch: 0064 loss_train: 0.1227\n",
      "Epoch: 0065 loss_train: 0.1045\n",
      "Epoch: 0066 loss_train: 0.1109\n",
      "Epoch: 0067 loss_train: 0.0922\n",
      "Epoch: 0068 loss_train: 0.1090\n",
      "Epoch: 0069 loss_train: 0.1168\n",
      "Epoch: 0070 loss_train: 0.1028\n",
      "Epoch: 0071 loss_train: 0.1100\n",
      "Epoch: 0072 loss_train: 0.1043\n",
      "Epoch: 0073 loss_train: 0.1028\n",
      "Epoch: 0074 loss_train: 0.1098\n",
      "Epoch: 0075 loss_train: 0.1235\n",
      "Epoch: 0076 loss_train: 0.1077\n",
      "Epoch: 0077 loss_train: 0.0956\n",
      "Epoch: 0078 loss_train: 0.1075\n",
      "Epoch: 0079 loss_train: 0.0906\n",
      "Epoch: 0080 loss_train: 0.1187\n",
      "Epoch: 0081 loss_train: 0.1013\n",
      "Epoch: 0082 loss_train: 0.0934\n",
      "Epoch: 0083 loss_train: 0.0932\n",
      "Epoch: 0084 loss_train: 0.0995\n",
      "Epoch: 0085 loss_train: 0.1055\n",
      "Epoch: 0086 loss_train: 0.0938\n",
      "Epoch: 0087 loss_train: 0.0789\n",
      "Epoch: 0088 loss_train: 0.1037\n",
      "Epoch: 0089 loss_train: 0.1003\n",
      "Epoch: 0090 loss_train: 0.1126\n",
      "Epoch: 0091 loss_train: 0.1252\n",
      "Epoch: 0092 loss_train: 0.1106\n",
      "Epoch: 0093 loss_train: 0.1296\n",
      "Epoch: 0094 loss_train: 0.0996\n",
      "Epoch: 0095 loss_train: 0.1221\n",
      "Epoch: 0096 loss_train: 0.0946\n",
      "Epoch: 0097 loss_train: 0.0792\n",
      "Epoch: 0098 loss_train: 0.0710\n",
      "Epoch: 0099 loss_train: 0.0899\n",
      "Epoch: 0100 loss_train: 0.1143\n",
      "Epoch: 0101 loss_train: 0.1300\n",
      "Epoch: 0102 loss_train: 0.1194\n",
      "Epoch: 0103 loss_train: 0.1030\n",
      "Epoch: 0104 loss_train: 0.1177\n",
      "Epoch: 0105 loss_train: 0.0753\n",
      "Epoch: 0106 loss_train: 0.0858\n",
      "Epoch: 0107 loss_train: 0.0915\n",
      "Epoch: 0108 loss_train: 0.1145\n",
      "Epoch: 0109 loss_train: 0.1142\n",
      "Epoch: 0110 loss_train: 0.1052\n",
      "Epoch: 0111 loss_train: 0.0965\n",
      "Epoch: 0112 loss_train: 0.0980\n",
      "Epoch: 0113 loss_train: 0.0976\n",
      "Epoch: 0114 loss_train: 0.1039\n",
      "Epoch: 0115 loss_train: 0.0715\n",
      "Epoch: 0116 loss_train: 0.0991\n",
      "Epoch: 0117 loss_train: 0.0962\n",
      "Epoch: 0118 loss_train: 0.1045\n",
      "Epoch: 0119 loss_train: 0.1296\n",
      "Epoch: 0120 loss_train: 0.0952\n",
      "Epoch: 0121 loss_train: 0.0834\n",
      "Epoch: 0122 loss_train: 0.1025\n",
      "Epoch: 0123 loss_train: 0.1017\n",
      "Epoch: 0124 loss_train: 0.0745\n",
      "Epoch: 0125 loss_train: 0.1083\n",
      "Epoch: 0126 loss_train: 0.1009\n",
      "Epoch: 0127 loss_train: 0.1445\n",
      "Epoch: 0128 loss_train: 0.1356\n",
      "Epoch: 0129 loss_train: 0.1262\n",
      "Epoch: 0130 loss_train: 0.1244\n",
      "Epoch: 0131 loss_train: 0.1055\n",
      "Epoch: 0132 loss_train: 0.1068\n",
      "Epoch: 0133 loss_train: 0.1123\n",
      "Epoch: 0134 loss_train: 0.0964\n",
      "Epoch: 0135 loss_train: 0.1128\n",
      "Epoch: 0136 loss_train: 0.1131\n",
      "Epoch: 0137 loss_train: 0.0824\n",
      "Epoch: 0138 loss_train: 0.1232\n",
      "Epoch: 0139 loss_train: 0.0927\n",
      "Epoch: 0140 loss_train: 0.1012\n",
      "Epoch: 0141 loss_train: 0.1110\n",
      "Epoch: 0142 loss_train: 0.0960\n",
      "Epoch: 0143 loss_train: 0.1166\n",
      "Epoch: 0144 loss_train: 0.1141\n",
      "Epoch: 0145 loss_train: 0.0970\n",
      "Epoch: 0146 loss_train: 0.0995\n",
      "Epoch: 0147 loss_train: 0.1204\n",
      "Epoch: 0148 loss_train: 0.0966\n",
      "Epoch: 0149 loss_train: 0.1396\n",
      "Epoch: 0150 loss_train: 0.1299\n",
      "Epoch: 0151 loss_train: 0.1399\n",
      "Epoch: 0152 loss_train: 0.0904\n",
      "Epoch: 0153 loss_train: 0.1213\n",
      "Epoch: 0154 loss_train: 0.1200\n",
      "Epoch: 0155 loss_train: 0.1318\n",
      "Epoch: 0156 loss_train: 0.1368\n",
      "Epoch: 0157 loss_train: 0.0923\n",
      "Epoch: 0158 loss_train: 0.1058\n",
      "Epoch: 0159 loss_train: 0.0803\n",
      "Epoch: 0160 loss_train: 0.1219\n",
      "Epoch: 0161 loss_train: 0.1080\n",
      "Epoch: 0162 loss_train: 0.0807\n",
      "Epoch: 0163 loss_train: 0.0824\n",
      "Epoch: 0164 loss_train: 0.1147\n",
      "Epoch: 0165 loss_train: 0.0816\n",
      "Epoch: 0166 loss_train: 0.1044\n",
      "Epoch: 0167 loss_train: 0.0811\n",
      "Epoch: 0168 loss_train: 0.0914\n",
      "Epoch: 0169 loss_train: 0.0946\n",
      "Epoch: 0170 loss_train: 0.1037\n",
      "Epoch: 0171 loss_train: 0.1196\n",
      "Epoch: 0172 loss_train: 0.0991\n",
      "Epoch: 0173 loss_train: 0.1030\n",
      "Epoch: 0174 loss_train: 0.0850\n",
      "Epoch: 0175 loss_train: 0.1207\n",
      "Epoch: 0176 loss_train: 0.1068\n",
      "Epoch: 0177 loss_train: 0.0905\n",
      "Epoch: 0178 loss_train: 0.0951\n",
      "Epoch: 0179 loss_train: 0.1032\n",
      "Epoch: 0180 loss_train: 0.1044\n",
      "Epoch: 0181 loss_train: 0.1402\n",
      "Epoch: 0182 loss_train: 0.0894\n",
      "Epoch: 0183 loss_train: 0.0948\n",
      "Epoch: 0184 loss_train: 0.1154\n",
      "Epoch: 0185 loss_train: 0.0807\n",
      "Epoch: 0186 loss_train: 0.0893\n",
      "Epoch: 0187 loss_train: 0.1022\n",
      "Epoch: 0188 loss_train: 0.0908\n",
      "Epoch: 0189 loss_train: 0.0932\n",
      "Epoch: 0190 loss_train: 0.0937\n",
      "Epoch: 0191 loss_train: 0.1177\n",
      "Epoch: 0192 loss_train: 0.1100\n",
      "Epoch: 0193 loss_train: 0.0731\n",
      "Epoch: 0194 loss_train: 0.1136\n",
      "Epoch: 0195 loss_train: 0.1134\n",
      "Epoch: 0196 loss_train: 0.0882\n",
      "Epoch: 0197 loss_train: 0.0825\n",
      "Epoch: 0198 loss_train: 0.0979\n",
      "Epoch: 0199 loss_train: 0.0998\n",
      "Epoch: 0200 loss_train: 0.1100\n",
      "Test set results: Loss: 1.0311 Accuracy: 0.7100 Accuracy_train: 0.9975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Create a list of indices for all nodes\n",
    "num_nodes = features.shape[0]  # total number of nodes (e.g., 496)\n",
    "all_indices = np.arange(num_nodes)\n",
    "\n",
    "# 2. Perform an 80/20 train-test split using scikit-learn\n",
    "idx_train, idx_test = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally convert indices to a PyTorch tensor if needed:\n",
    "idx_train = torch.tensor(idx_train, dtype=torch.long)\n",
    "idx_test = torch.tensor(idx_test, dtype=torch.long)\n",
    "\n",
    "# 3. Training function (uses global idx_train)\n",
    "def train(epoch):\n",
    "    model.train()                       # Set model to training mode\n",
    "    optimizer.zero_grad()               # Clear gradients\n",
    "    output = model(features, adj_normalized)  # Forward pass\n",
    "    # Compute training loss only on the training indices\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()               # Backward pass: compute gradients\n",
    "    optimizer.step()                    # Update model parameters\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()))\n",
    "\n",
    "# 4. Testing/evaluation function (uses global idx_test)\n",
    "def test():\n",
    "    model.eval()                        # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj_normalized)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        # Compute predictions for test nodes\n",
    "        pred = output[idx_test].max(1)[1]\n",
    "        pred_train= output[idx_train].max(1)[1]\n",
    "        # Calculate accuracy on test set\n",
    "        correct = pred.eq(labels[idx_test]).sum().item()\n",
    "        accuracy = correct / len(idx_test)\n",
    "        correct_train= pred_train.eq(labels[idx_train]).sum().item()\n",
    "        accuracy_train = correct_train / len(idx_train)\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"Loss: {:.4f}\".format(loss_test.item()),\n",
    "          \"Accuracy: {:.4f}\".format(accuracy),\n",
    "         \"Accuracy_train: {:.4f}\".format(accuracy_train))\n",
    "\n",
    "# 5. Training loop\n",
    "num_epochs = 200  # Adjust the number of epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "\n",
    "# 6. Evaluate on the test set after training\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307738f-8031-447a-b0f2-2fa2031873d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
